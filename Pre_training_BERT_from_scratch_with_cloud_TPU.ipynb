{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre_training_BERT_from_scratch_with_cloud_TPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverstar0727/Today_I_Learned/blob/master/Pre_training_BERT_from_scratch_with_cloud_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Pre-training BERT from scratch with cloud TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZPgpRl5g2e2"
      },
      "source": [
        "In this experiment, we will be pre-training a state-of-the-art Natural Language Understanding model [BERT](https://arxiv.org/abs/1810.04805.) on arbitrary text data using Google Cloud infrastructure.\n",
        "\n",
        "This guide covers all stages of the procedure, including:\n",
        "\n",
        "1. Setting up the training environment\n",
        "2. Downloading raw text data\n",
        "3. Preprocessing text data\n",
        "4. Learning a new vocabulary\n",
        "5. Creating sharded pre-training data\n",
        "6. Setting up GCS storage for data and model\n",
        "7. Training the model on a cloud TPU\n",
        "\n",
        "For persistent storage of training data and model, you will require a Google Cloud Storage bucket. \n",
        "Please follow the [Google Cloud TPU quickstart](https://cloud.google.com/tpu/docs/quickstart) to create a GCP account and GCS bucket. New Google Cloud users have [$300 free credit](https://cloud.google.com/free/) to get started with any GCP product. \n",
        "\n",
        "Steps 1-5 of this tutorial can be run without a GCS bucket for demonstration purposes. In that case, however, you will not be able to train the model.\n",
        "\n",
        "**Note** \n",
        "The only parameter you *really have to set* is BUCKET_NAME in steps 5 and 6. Everything else has default values which should work for most use-cases.\n",
        "\n",
        "**Note** \n",
        "Pre-training a BERT-Base model on a TPUv2 will take about 54 hours. Google Colab is not designed for executing such long-running jobs and will interrupt the training process every 8 hours or so. For uninterrupted training, consider using a preemptible TPUv2 instance. \n",
        "\n",
        "That said, at the time of writing (09.05.2019), with a Colab TPU, pre-training a BERT model from scratch can be achieved at a negligible cost of storing the said model and data in GCS  (~1 USD).\n",
        "\n",
        "Now, let's get to business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODimOhBR05yR"
      },
      "source": [
        "MIT License\n",
        "\n",
        "Copyright (c) [2019] [Antyukhov Denis Olegovich]\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjad5jsr9YaM"
      },
      "source": [
        "## Step 1: setting up training environment\n",
        "First and foremost, we get the packages required to train the model. \n",
        "The Jupyter environment allows executing bash commands directly from the notebook by using an exclamation mark ‘!’. I will be exploiting this approach to make use of several other bash commands throughout the experiment.\n",
        "\n",
        "Now, let’s import the packages and authorize ourselves in Google Cloud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "0f53498c-610a-40fa-eb2b-7d939a37398e"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!git clone https://github.com/google-research/bert\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import nltk\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import sentencepiece as spm\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "sys.path.append(\"bert\")\n",
        "\n",
        "from bert import modeling, optimization, tokenization\n",
        "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
        "\n",
        "auth.authenticate_user()\n",
        "  \n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "# create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
        "sh = logging.StreamHandler()\n",
        "sh.setLevel(logging.INFO)\n",
        "sh.setFormatter(formatter)\n",
        "log.handlers = [sh]\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  USE_TPU = True\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "  USE_TPU = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.83\n",
            "Cloning into 'bert'...\n",
            "remote: Enumerating objects: 333, done.\u001b[K\n",
            "remote: Total 333 (delta 0), reused 0 (delta 0), pack-reused 333\u001b[K\n",
            "Receiving objects: 100% (333/333), 282.45 KiB | 3.82 MiB/s, done.\n",
            "Resolving deltas: 100% (183/183), done.\n",
            "WARNING:tensorflow:From /content/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-18 12:09:36,959 :  Using TPU runtime\n",
            "2019-09-18 12:09:36,961 :  TPU address is grpc://10.91.210.154:8470\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvfNt0mVZ33n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGVXMoC-aMy1"
      },
      "source": [
        "## Step 2: getting the data\n",
        "\n",
        "We begin with obtaining a corpus of raw text data. For this experiment, we will be using the [OpenSubtitles](http://www.opensubtitles.org/) dataset, which is available for 65 languages [here](http://opus.nlpl.eu/OpenSubtitles-v2016.php). \n",
        "\n",
        "Unlike more common text datasets (like Wikipedia) it does not require any complex pre-processing. It also comes pre-formatted with one sentence per line.\n",
        "\n",
        "Feel free to use the dataset for your language instead by changing the language code (en) below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FotFkkshbdvK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "eaab8afd-053b-4a5f-cd0c-69b65ce43004"
      },
      "source": [
        "AVAILABLE =  {'af','ar','bg','bn','br','bs','ca','cs',\n",
        "              'da','de','el','en','eo','es','et','eu',\n",
        "              'fa','fi','fr','gl','he','hi','hr','hu',\n",
        "              'hy','id','is','it','ja','ka','kk','ko',\n",
        "              'lt','lv','mk','ml','ms','nl','no','pl',\n",
        "              'pt','pt_br','ro','ru','si','sk','sl','sq',\n",
        "              'sr','sv','ta','te','th','tl','tr','uk',\n",
        "              'ur','vi','ze_en','ze_zh','zh','zh_cn',\n",
        "              'zh_en','zh_tw','zh_zh'}\n",
        "\n",
        "LANG_CODE = \"en\" #@param {type:\"string\"}\n",
        "\n",
        "assert LANG_CODE in AVAILABLE, \"Invalid language code selected\"\n",
        "\n",
        "!wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.'$LANG_CODE'.gz -O dataset.txt.gz\n",
        "!gzip -d dataset.txt.gz\n",
        "!tail dataset.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-18 12:10:08--  http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.en.gz\n",
            "Resolving opus.nlpl.eu (opus.nlpl.eu)... 193.166.25.9\n",
            "Connecting to opus.nlpl.eu (opus.nlpl.eu)|193.166.25.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/en.txt.gz [following]\n",
            "--2019-09-18 12:10:09--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/en.txt.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.1, 86.50.254.0\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2906709304 (2.7G) [application/gzip]\n",
            "Saving to: ‘dataset.txt.gz’\n",
            "\n",
            "dataset.txt.gz      100%[===================>]   2.71G  20.5MB/s    in 2m 18s  \n",
            "\n",
            "2019-09-18 12:12:28 (20.1 MB/s) - ‘dataset.txt.gz’ saved [2906709304/2906709304]\n",
            "\n",
            "The astronomers run at full speed turning around each time they are pressed too closely and reducing the fragile beings to dust.\n",
            "At last, the astronomers have found the shell and quickly shut themselves in the interior.\n",
            "Thanks to the advance, they have succeeded in getting over their adversaries.\n",
            "Only one, the president, has been left behind.\n",
            "He rushes to the rope which hangs from the point of the shell and letting himself slide down the rope he gives it an impetus which causes the shell to fall off the edge of the moon.\n",
            "The shell falls with sickening rapidity.\n",
            "The sea appears.\n",
            "The shell balances and thanks to the hermetically sealed air in its interior rises slowly to the surface.\n",
            "The shell is picked up by a steamer which tows it to port.\n",
            "The Mayor welcomes the astronomers with a speech, and the general ovation awaits their happy return.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb5TPsPOppn0"
      },
      "source": [
        "For demonstration purposes, we will only use a small fraction of the whole corpus for this experiment. \n",
        "\n",
        "When training the real model, make sure to uncheck the DEMO_MODE checkbox to use a 100x larger dataset.\n",
        "\n",
        "Rest assured, 100M lines are perfectly sufficient to train a reasonably good BERT-base model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDR5Z1MDgB1g"
      },
      "source": [
        "DEMO_MODE = True #@param {type:\"boolean\"}\n",
        "\n",
        "if DEMO_MODE:\n",
        "  CORPUS_SIZE = 1000000\n",
        "else:\n",
        "  CORPUS_SIZE = 100000000 #@param {type: \"integer\"}\n",
        "  \n",
        "!(head -n $CORPUS_SIZE dataset.txt) > subdataset.txt\n",
        "!mv subdataset.txt dataset.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRQd4-v0nQqH"
      },
      "source": [
        "## Step 3: preprocessing text\n",
        "\n",
        "The raw text data we have downloaded contains punсtuation, uppercase letters and non-UTF symbols which we will remove before proceeding. During inference, we will apply the same normalization procedure to new data.\n",
        "\n",
        "If your use-case requires different preprocessing (e.g. if uppercase letters or punctuation are expected during inference), feel free to modify the function below to accomodate for your needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCi2oSdInRkC"
      },
      "source": [
        "regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
        "\n",
        "def normalize_text(text):\n",
        "  # lowercase text\n",
        "  text = str(text).lower()\n",
        "  # remove non-UTF\n",
        "  text = text.encode(\"utf-8\", \"ignore\").decode()\n",
        "  # remove punktuation symbols\n",
        "  text = \" \".join(regex_tokenizer.tokenize(text))\n",
        "  return text\n",
        "\n",
        "def count_lines(filename):\n",
        "  count = 0\n",
        "  with open(filename) as fi:\n",
        "    for line in fi:\n",
        "      count += 1\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYtwtDnesaQQ"
      },
      "source": [
        "Check how that works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gngtEZWqVhY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "785333c0-6d5f-4d24-e4fc-7eb0fe5094d0"
      },
      "source": [
        "normalize_text('Thanks to the advance, they have succeeded in getting over their adversaries.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thanks to the advance they have succeeded in getting over their adversaries'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY-Kvnx6sUFS"
      },
      "source": [
        "Apply normalization to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myjxQe5awo1v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80757166-7e12-44d5-b79a-69da6e6f6acd"
      },
      "source": [
        "RAW_DATA_FPATH = \"dataset.txt\" #@param {type: \"string\"}\n",
        "PRC_DATA_FPATH = \"proc_dataset.txt\" #@param {type: \"string\"}\n",
        "\n",
        "# apply normalization to the dataset\n",
        "# this will take a minute or two\n",
        "\n",
        "total_lines = count_lines(RAW_DATA_FPATH)\n",
        "bar = Progbar(total_lines)\n",
        "\n",
        "with open(RAW_DATA_FPATH,encoding=\"utf-8\") as fi:\n",
        "  with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
        "    for l in fi:\n",
        "      fo.write(normalize_text(l)+\"\\n\")\n",
        "      bar.add(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000000/1000000 [==============================] - 5s 5us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3A64RZjwo9k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVO9EnUwrluQ"
      },
      "source": [
        "## Step 4: building the vocabulary\n",
        "\n",
        "For the next step, we will learn a new vocabulary that we will use to represent our dataset. \n",
        "\n",
        "The BERT paper uses a WordPiece tokenizer, which is not available in opensource. Instead, we will be using SentencePiece tokenizer in unigram mode. While it is not directly compatible with BERT, with a small hack we can make it work.\n",
        "\n",
        "SentencePiece requires quite a lot of RAM, so running it on the full dataset in Colab will crash the kernel. To avoid this, we will randomly subsample a fraction of the dataset for building the vocabulary. Another option would be to use a machine with more RAM for this step - that decision is up to you.\n",
        "\n",
        "Also, SentencePiece adds BOS and EOS control symbols to the vocabulary by default. We disable them explicitly by setting their indices to -1.\n",
        "\n",
        "The typical values for VOC_SIZE are somewhere in between 32000 and 128000. We reserve NUM_PLACEHOLDERS tokens in case one wants to update the vocabulary and fine-tune the model after the pre-training phase is finished."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18nn6eW_s-fV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fe0d4bb-d3a5-49ec-f1f6-4169b353ef8e"
      },
      "source": [
        "MODEL_PREFIX = \"tokenizer\" #@param {type: \"string\"}\n",
        "VOC_SIZE = 32000 #@param {type:\"integer\"}\n",
        "SUBSAMPLE_SIZE = 12800000 #@param {type:\"integer\"}\n",
        "NUM_PLACEHOLDERS = 256 #@param {type:\"integer\"}\n",
        "\n",
        "SPM_COMMAND = ('--input={} --model_prefix={} '\n",
        "               '--vocab_size={} --input_sentence_size={} '\n",
        "               '--shuffle_input_sentence=true ' \n",
        "               '--bos_id=-1 --eos_id=-1').format(\n",
        "               PRC_DATA_FPATH, MODEL_PREFIX, \n",
        "               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(SPM_COMMAND)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAowCoH2u1iZ"
      },
      "source": [
        "Now let's see how we can make SentencePiece tokenizer work for the BERT model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OLaw7kPW3he"
      },
      "source": [
        "Below is a sentence tokenized using the WordPiece vocabulary from a pretrained English [BERT-base](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip) model from the official [repo](https://github.com/google-research/bert). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwHAp_Gh5OPK"
      },
      "source": [
        "testcase = \"Colorless geothermal substations are generating furiously\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyEGAVl_5YRD"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        ">>> wordpiece.tokenize(\"Colorless geothermal substations are generating furiously\")\n",
        "\n",
        "['color',\n",
        " '##less',\n",
        " 'geo',\n",
        " '##thermal',\n",
        " 'sub',\n",
        " '##station',\n",
        " '##s',\n",
        " 'are',\n",
        " 'generating',\n",
        " 'furiously']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNiLdWXTh9cj"
      },
      "source": [
        "As we can see, the WordPiece tokenizer prepends the subwords which occur in the middle of words with '##'. The subwords occurring at the beginning of words are unchanged. If the subword occurs both in the beginning and in the middle of words, both versions (with and without '##') are added to the vocabulary.\n",
        "\n",
        "Now let's have a look at the vocabulary that the SentencePiece tokenizer has learned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8_ebLxqTnWu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bcb062f3-5261-42d1-9617-205b6bc6d6ff"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  dataset.txt\t    sample_data      tokenizer.vocab\n",
            "bert\t  proc_dataset.txt  tokenizer.model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYlBqiv5UD-j"
      },
      "source": [
        "SentencePiece has created two files: tokenizer.model and tokenizer.vocab. Let's have a look at the learned vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDJ9QmNMUEQf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46d5f5d7-fdb6-4a08-b1d6-82bfb04471b2"
      },
      "source": [
        "!head -n 100 tokenizer.vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<unk>\t0\n",
            "▁you\t-3.2342\n",
            "▁i\t-3.2821\n",
            "▁the\t-3.56375\n",
            "▁s\t-3.84955\n",
            "▁to\t-3.87601\n",
            "▁a\t-3.9102\n",
            "▁it\t-3.97593\n",
            "▁t\t-4.25729\n",
            "▁and\t-4.32686\n",
            "▁that\t-4.33991\n",
            "▁of\t-4.57503\n",
            "▁what\t-4.59504\n",
            "▁is\t-4.67144\n",
            "▁me\t-4.7115\n",
            "▁in\t-4.71265\n",
            "▁we\t-4.7231\n",
            "▁he\t-4.88446\n",
            "▁this\t-4.89466\n",
            "▁no\t-4.98519\n",
            "▁on\t-4.99226\n",
            "▁my\t-4.99616\n",
            "▁m\t-5.02169\n",
            "▁your\t-5.02715\n",
            "▁for\t-5.05648\n",
            "▁have\t-5.06695\n",
            "▁don\t-5.09057\n",
            "▁do\t-5.12828\n",
            "▁\t-5.12998\n",
            "▁re\t-5.1371\n",
            "▁can\t-5.20534\n",
            "▁was\t-5.20891\n",
            "▁know\t-5.22494\n",
            "▁be\t-5.23557\n",
            "▁are\t-5.25247\n",
            "▁not\t-5.26685\n",
            "▁all\t-5.29439\n",
            "▁with\t-5.31925\n",
            "▁but\t-5.37188\n",
            "▁so\t-5.42124\n",
            "▁get\t-5.43031\n",
            "▁here\t-5.43446\n",
            "▁just\t-5.43719\n",
            "▁ll\t-5.49322\n",
            "▁like\t-5.5182\n",
            "▁they\t-5.5196\n",
            "▁there\t-5.52012\n",
            "▁up\t-5.55201\n",
            "▁go\t-5.55369\n",
            "▁she\t-5.5887\n",
            "▁right\t-5.61613\n",
            "▁out\t-5.64626\n",
            "▁oh\t-5.68517\n",
            "s\t-5.71583\n",
            "▁come\t-5.73109\n",
            "▁if\t-5.73944\n",
            "▁him\t-5.74635\n",
            "▁one\t-5.75141\n",
            "▁about\t-5.75416\n",
            "▁got\t-5.77074\n",
            "▁at\t-5.78535\n",
            "▁now\t-5.8265\n",
            "▁yeah\t-5.84144\n",
            "▁how\t-5.84948\n",
            "▁her\t-5.87603\n",
            "▁well\t-5.94933\n",
            "▁let\t-5.95131\n",
            "▁good\t-5.98986\n",
            "▁want\t-6.01295\n",
            "▁ve\t-6.02348\n",
            "▁think\t-6.04507\n",
            "▁who\t-6.06409\n",
            "▁did\t-6.09598\n",
            "▁see\t-6.10352\n",
            "▁why\t-6.11416\n",
            "▁will\t-6.12599\n",
            "▁gonna\t-6.17628\n",
            "▁from\t-6.17938\n",
            "▁look\t-6.19767\n",
            "▁as\t-6.20588\n",
            "▁yes\t-6.21377\n",
            "▁back\t-6.22989\n",
            "▁d\t-6.23892\n",
            "▁his\t-6.24827\n",
            "n\t-6.25315\n",
            "▁man\t-6.25455\n",
            "▁when\t-6.26481\n",
            "▁okay\t-6.27249\n",
            "▁time\t-6.27638\n",
            "▁could\t-6.28472\n",
            "▁take\t-6.30417\n",
            "▁hey\t-6.31565\n",
            "▁say\t-6.35372\n",
            "▁had\t-6.37539\n",
            "▁an\t-6.37643\n",
            "▁us\t-6.38685\n",
            "▁or\t-6.38714\n",
            "▁were\t-6.38932\n",
            "▁some\t-6.39201\n",
            "▁where\t-6.39465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBsURk_h5jw4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "80b23dbe-86e3-4609-9ccd-7037d2740949"
      },
      "source": [
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learnt vocab size: 31743\n",
            "Sample tokens: ['▁beth', '▁voice', '▁hologram', '▁condom', '▁hail', '▁regret', '▁satir', '▁entering', '▁admit', '▁marion']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJPtxrtz5470"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YjqHVRpmlKq"
      },
      "source": [
        "As we may observe, SentencePiece does quite the opposite to WordPiece. From the [documentation](https://github.com/google/sentencepiece/blob/master/README.md):\n",
        "\n",
        "\n",
        "SentencePiece first escapes the whitespace with a meta-symbol \"▁\" (U+2581) as follows:\n",
        "\n",
        "`Hello▁World`.\n",
        "\n",
        "Then, this text is segmented into small pieces, for example:\n",
        "\n",
        "`[Hello] [▁Wor] [ld] [.]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD-28l_0p0PQ"
      },
      "source": [
        "Subwords which occur after whitespace (which are also those that most words begin with) are prepended with '▁', while others are unchanged. This excludes subwords which only occur at the beginning of sentences and nowhere else. These cases should be quite rare, however. \n",
        "\n",
        "So, in order to obtain a vocabulary analogous to WordPiece, we need to perform a simple conversion, removing \"▁\" from the tokens that contain it and adding \"##\"  to the ones that don't."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QJGFjzOMbfN"
      },
      "source": [
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64dcVgD98S28"
      },
      "source": [
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL9ZR3RN9IMA"
      },
      "source": [
        "We also add some special control symbols which are required by the BERT architecture. By convention, we put those at the beginning of the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdTlXDPL8cHE"
      },
      "source": [
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry1jvEr9EIdd"
      },
      "source": [
        "We also append some placeholder tokens to the vocabulary. Those are useful if one wishes to update the pre-trained model with new, task-specific tokens. \n",
        "\n",
        "In that case, the placeholder tokens are replaced with new real ones, the pre-training data is re-generated, and the model is fine-tuned on new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLYSTil4E0Dm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c5d823e-6603-40f6-b43c-0810d0594500"
      },
      "source": [
        "bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJKk_7JI-MtW"
      },
      "source": [
        "Finally, we write the obtained vocabulary to file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G1jg0cj9Duf"
      },
      "source": [
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5fs7H049nB8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MqXZnc3FCuY"
      },
      "source": [
        "Now let's see how the new vocabulary works in practice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsSOnEnC-jG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d2c8aeaa-cb8a-4e17-d853-55ea1edcc0e3"
      },
      "source": [
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "bert_tokenizer.tokenize(testcase)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['color',\n",
              " '##less',\n",
              " 'geo',\n",
              " '##ther',\n",
              " '##mal',\n",
              " 'subs',\n",
              " '##tation',\n",
              " '##s',\n",
              " 'are',\n",
              " 'generat',\n",
              " '##ing',\n",
              " 'furious',\n",
              " '##ly']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN8xDNfF0Q2q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauD8ndhEA-z"
      },
      "source": [
        "Looking good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwFtStCo__QX"
      },
      "source": [
        "## Step 5: generating pre-training data\n",
        "\n",
        "With the vocabulary at hand, we are ready to generate pre-training data for the BERT model. Since our dataset might be quite large, we will split it into shards:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyN1nI04-uKV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5f83572-7cd4-4a63-e03d-7a3669ea2101"
      },
      "source": [
        "!mkdir ./shards\n",
        "!split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_\n",
        "!ls ./shards/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shard_0000  shard_0001\tshard_0002  shard_0003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-FSq3zNFvLs"
      },
      "source": [
        "Before we start generating, we need to set some model-specific parameters.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnZcD0yIBGPd"
      },
      "source": [
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "DO_LOWER_CASE = True #@param {type:\"boolean\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-MibOBkFam2"
      },
      "source": [
        "Now, for each shard we need to call *create_pretraining_data.py* script. To that end, we will employ the  *xargs* command. \n",
        "\n",
        "Running this might take quite some time depending on the size of your dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbZjIeVP0T36"
      },
      "source": [
        "XARGS_CMD = (\"ls ./shards/ | \"\n",
        "             \"xargs -n 1 -P {} -I{} \"\n",
        "             \"python3 bert/create_pretraining_data.py \"\n",
        "             \"--input_file=./shards/{} \"\n",
        "             \"--output_file={}/{}.tfrecord \"\n",
        "             \"--vocab_file={} \"\n",
        "             \"--do_lower_case={} \"\n",
        "             \"--max_predictions_per_seq={} \"\n",
        "             \"--max_seq_length={} \"\n",
        "             \"--masked_lm_prob={} \"\n",
        "             \"--random_seed=34 \"\n",
        "             \"--dupe_factor=5\")\n",
        "\n",
        "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n",
        "                             VOC_FNAME, DO_LOWER_CASE, \n",
        "                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyo9_LcQ0pla"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5XzaY8xCdiV"
      },
      "source": [
        "tf.gfile.MkDir(PRETRAINING_DIR)\n",
        "!$XARGS_CMD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKnW_hVxPoP6"
      },
      "source": [
        "## Step 6: setting up persistent storage\n",
        "\n",
        "To preserve our hard-earned assets, we will persist them to Google Cloud Storage. Provided that you have created the GCS bucket, this should be simple.\n",
        "\n",
        "We will create two directories in GCS, one for the data and one for the model.\n",
        "In the model directory, we will put the model vocabulary and configuration file.\n",
        "\n",
        "**Configure your BUCKET_NAME variable here before proceeding, otherwise the model and data will not be saved.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDtrt68QQIHs"
      },
      "source": [
        "BUCKET_NAME = \"bert_resourses\" #@param {type:\"string\"}\n",
        "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
        "tf.gfile.MkDir(MODEL_DIR)\n",
        "\n",
        "if not BUCKET_NAME:\n",
        "  log.warning(\"WARNING: BUCKET_NAME is not set. \"\n",
        "              \"You will not be able to train the model.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YigCcV-hSHVH"
      },
      "source": [
        "Below is the sample hyperparameter configuration for BERT-base. Change at your own risk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEpSGpUKReKW"
      },
      "source": [
        "# use this for BERT-base\n",
        "\n",
        "bert_base_config = {\n",
        "  \"attention_probs_dropout_prob\": 0.1, \n",
        "  \"directionality\": \"bidi\", \n",
        "  \"hidden_act\": \"gelu\", \n",
        "  \"hidden_dropout_prob\": 0.1, \n",
        "  \"hidden_size\": 768, \n",
        "  \"initializer_range\": 0.02, \n",
        "  \"intermediate_size\": 3072, \n",
        "  \"max_position_embeddings\": 512, \n",
        "  \"num_attention_heads\": 12, \n",
        "  \"num_hidden_layers\": 12, \n",
        "  \"pooler_fc_size\": 768, \n",
        "  \"pooler_num_attention_heads\": 12, \n",
        "  \"pooler_num_fc_layers\": 3, \n",
        "  \"pooler_size_per_head\": 128, \n",
        "  \"pooler_type\": \"first_token_transform\", \n",
        "  \"type_vocab_size\": 2, \n",
        "  \"vocab_size\": VOC_SIZE\n",
        "}\n",
        "\n",
        "with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
        "  json.dump(bert_base_config, fo, indent=2)\n",
        "  \n",
        "with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txgrEDugRG48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5227e9f0-6ccb-48cf-9469-b7e6e10e25ed"
      },
      "source": [
        "if BUCKET_NAME:\n",
        "  !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://pretraining_data/shard_0003.tfrecord [Content-Type=application/octet-stream]...\n",
            "Copying file://bert_model/bert_config.json [Content-Type=application/json]...\n",
            "Copying file://pretraining_data/shard_0002.tfrecord [Content-Type=application/octet-stream]...\n",
            "Copying file://pretraining_data/shard_0001.tfrecord [Content-Type=application/octet-stream]...\n",
            "Copying file://pretraining_data/shard_0000.tfrecord [Content-Type=application/octet-stream]...\n",
            "Copying file://bert_model/vocab.txt [Content-Type=text/plain]...\n",
            "-\n",
            "Operation completed over 6 objects/269.4 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DL6xuCAYPrp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdQEOzhYmSh"
      },
      "source": [
        "## Step 7: training the model\n",
        "\n",
        "We are almost ready to begin training our model. If you wish  to continue an interrupted training run, you may skip steps 2-6 and proceed from here.\n",
        "\n",
        "**Make sure that you have set the BUCKET_NAME here as well.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXAuzsJfYrio",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ae5e2ddc-bdf8-4ed7-bcff-82f784169db4"
      },
      "source": [
        "BUCKET_NAME = \"bert_resourses\" #@param {type:\"string\"}\n",
        "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "# Input data pipeline config\n",
        "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "\n",
        "# Training procedure config\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 2e-5\n",
        "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
        "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "if BUCKET_NAME:\n",
        "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "else:\n",
        "  BUCKET_PATH = \".\"\n",
        "\n",
        "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
        "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
        "\n",
        "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
        "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n",
        "\n",
        "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
        "\n",
        "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "log.info(\"Using {} data shards\".format(len(input_files)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-18 12:40:05,216 :  Couldn't match files for checkpoint gs://bert_resourses/bert_model/model.ckpt-102500\n",
            "2019-09-18 12:40:05,651 :  Using checkpoint: None\n",
            "2019-09-18 12:40:05,656 :  Using 4 data shards\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwwF-WqcZHUG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQTFjITdd53F"
      },
      "source": [
        "Prepare the training run configuration, build the estimator and input function, power up the bass cannon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMahsqUnZ55z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "1db95e73-228c-4134-d67d-5324cc68169e"
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=TRAIN_STEPS,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=USE_TPU,\n",
        "      use_one_hot_embeddings=True)\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=BERT_GCS_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=USE_TPU,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)\n",
        "  \n",
        "train_input_fn = input_fn_builder(\n",
        "        input_files=input_files,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "        is_training=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-18 12:42:14,316 :  Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f242273e7b8>) includes params argument, but params are not passed to Estimator.\n",
            "2019-09-18 12:42:14,325 :  Using config: {'_model_dir': 'gs://bert_resourses/bert_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.91.210.154:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f24285ab208>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.91.210.154:8470', '_evaluation_master': 'grpc://10.91.210.154:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2500, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f24285abc50>}\n",
            "2019-09-18 12:42:14,330 :  _TPUContext: eval_on_tpu True\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNt5ykopeIYB"
      },
      "source": [
        "Fire!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e3c3b40-2dfe-44a6-8fb6-fb3ce3f28cdf"
      },
      "source": [
        "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-18 12:42:19,833 :  Couldn't match files for checkpoint gs://bert_resourses/bert_model/model.ckpt-102500\n",
            "2019-09-18 12:42:19,835 :  Querying Tensorflow master (grpc://10.91.210.154:8470) for TPU system metadata.\n",
            "2019-09-18 12:42:19,857 :  Found TPU system:\n",
            "2019-09-18 12:42:19,859 :  *** Num TPU Cores: 8\n",
            "2019-09-18 12:42:19,860 :  *** Num TPU Workers: 1\n",
            "2019-09-18 12:42:19,861 :  *** Num TPU Cores Per Worker: 8\n",
            "2019-09-18 12:42:19,863 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 10112529597588573712)\n",
            "2019-09-18 12:42:19,866 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 7136981085282186828)\n",
            "2019-09-18 12:42:19,867 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 4902132816901732828)\n",
            "2019-09-18 12:42:19,868 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 2089294067726479014)\n",
            "2019-09-18 12:42:19,869 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 18174741405453929323)\n",
            "2019-09-18 12:42:19,871 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 15460803542009079250)\n",
            "2019-09-18 12:42:19,872 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 10830896127408992583)\n",
            "2019-09-18 12:42:19,873 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 11487562379932309715)\n",
            "2019-09-18 12:42:19,874 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10911185247063431966)\n",
            "2019-09-18 12:42:19,875 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 8988734776553969133)\n",
            "2019-09-18 12:42:19,876 :  *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7268548307044494427)\n",
            "2019-09-18 12:42:19,920 :  From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "2019-09-18 12:42:19,952 :  Calling model_fn.\n",
            "2019-09-18 12:42:19,954 :  From /content/bert/run_pretraining.py:337: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "2019-09-18 12:42:19,968 :  From /content/bert/run_pretraining.py:368: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "2019-09-18 12:42:19,969 :  From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "2019-09-18 12:42:20,023 :  From /content/bert/run_pretraining.py:385: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "2019-09-18 12:42:20,025 :  From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "2019-09-18 12:42:20,028 :  From /content/bert/run_pretraining.py:393: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "2019-09-18 12:42:20,052 :  From /content/bert/run_pretraining.py:400: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2019-09-18 12:42:20,154 :  From /content/bert/run_pretraining.py:117: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "2019-09-18 12:42:20,155 :  *** Features ***\n",
            "2019-09-18 12:42:20,156 :    name = input_ids, shape = (16, 128)\n",
            "2019-09-18 12:42:20,157 :    name = input_mask, shape = (16, 128)\n",
            "2019-09-18 12:42:20,158 :    name = masked_lm_ids, shape = (16, 20)\n",
            "2019-09-18 12:42:20,159 :    name = masked_lm_positions, shape = (16, 20)\n",
            "2019-09-18 12:42:20,160 :    name = masked_lm_weights, shape = (16, 20)\n",
            "2019-09-18 12:42:20,161 :    name = next_sentence_labels, shape = (16, 1)\n",
            "2019-09-18 12:42:20,162 :    name = segment_ids, shape = (16, 128)\n",
            "2019-09-18 12:42:20,163 :  From bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "2019-09-18 12:42:20,176 :  From bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "2019-09-18 12:42:20,225 :  From bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "2019-09-18 12:42:20,285 :  From bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "2019-09-18 12:42:20,313 :  From bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "2019-09-18 12:42:24,555 :  **** Trainable Variables ****\n",
            "2019-09-18 12:42:24,556 :    name = bert/embeddings/word_embeddings:0, shape = (32000, 768)\n",
            "2019-09-18 12:42:24,557 :    name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "2019-09-18 12:42:24,558 :    name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "2019-09-18 12:42:24,559 :    name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,560 :    name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,561 :    name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,562 :    name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,563 :    name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,564 :    name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,565 :    name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,566 :    name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,567 :    name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,568 :    name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,568 :    name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,569 :    name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,570 :    name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2019-09-18 12:42:24,571 :    name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "2019-09-18 12:42:24,572 :    name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "2019-09-18 12:42:24,572 :    name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,573 :    name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,574 :    name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,575 :    name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,576 :    name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,577 :    name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,577 :    name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,578 :    name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,579 :    name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,580 :    name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,581 :    name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,581 :    name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,582 :    name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,583 :    name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2019-09-18 12:42:24,584 :    name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "2019-09-18 12:42:24,585 :    name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "2019-09-18 12:42:24,586 :    name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,586 :    name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,587 :    name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,588 :    name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,589 :    name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,590 :    name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,591 :    name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,592 :    name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,592 :    name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,593 :    name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,594 :    name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,595 :    name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,596 :    name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,597 :    name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2019-09-18 12:42:24,597 :    name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "2019-09-18 12:42:24,598 :    name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "2019-09-18 12:42:24,599 :    name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,600 :    name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,601 :    name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,602 :    name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,603 :    name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,603 :    name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,604 :    name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,605 :    name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,606 :    name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,607 :    name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,608 :    name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,609 :    name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,609 :    name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,610 :    name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2019-09-18 12:42:24,611 :    name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "2019-09-18 12:42:24,612 :    name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "2019-09-18 12:42:24,613 :    name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,614 :    name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,615 :    name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,615 :    name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,616 :    name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,617 :    name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,618 :    name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,619 :    name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,620 :    name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,621 :    name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,622 :    name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,622 :    name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,623 :    name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,624 :    name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2019-09-18 12:42:24,625 :    name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "2019-09-18 12:42:24,626 :    name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "2019-09-18 12:42:24,627 :    name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,628 :    name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,629 :    name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,629 :    name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,630 :    name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,631 :    name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,632 :    name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,633 :    name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,634 :    name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,634 :    name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,635 :    name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,636 :    name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,637 :    name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,638 :    name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2019-09-18 12:42:24,639 :    name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "2019-09-18 12:42:24,639 :    name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "2019-09-18 12:42:24,640 :    name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,641 :    name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,642 :    name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,643 :    name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,644 :    name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,645 :    name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,645 :    name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,646 :    name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,647 :    name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,648 :    name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,649 :    name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,650 :    name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,650 :    name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,651 :    name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2019-09-18 12:42:24,652 :    name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "2019-09-18 12:42:24,653 :    name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "2019-09-18 12:42:24,654 :    name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,655 :    name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,656 :    name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,656 :    name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,657 :    name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,658 :    name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,659 :    name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,660 :    name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,661 :    name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,662 :    name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,663 :    name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,663 :    name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,664 :    name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,665 :    name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2019-09-18 12:42:24,666 :    name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "2019-09-18 12:42:24,667 :    name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "2019-09-18 12:42:24,668 :    name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,668 :    name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,669 :    name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,670 :    name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,671 :    name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,672 :    name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,673 :    name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,674 :    name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,675 :    name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,676 :    name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,676 :    name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,677 :    name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,678 :    name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,679 :    name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2019-09-18 12:42:24,680 :    name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "2019-09-18 12:42:24,681 :    name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "2019-09-18 12:42:24,682 :    name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,683 :    name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,684 :    name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,684 :    name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,685 :    name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,686 :    name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,687 :    name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,688 :    name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,689 :    name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,689 :    name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,690 :    name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,691 :    name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,692 :    name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,693 :    name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2019-09-18 12:42:24,694 :    name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "2019-09-18 12:42:24,695 :    name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "2019-09-18 12:42:24,696 :    name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,697 :    name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,697 :    name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,698 :    name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,699 :    name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,700 :    name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,701 :    name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,702 :    name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,702 :    name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,703 :    name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,704 :    name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,706 :    name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,707 :    name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,708 :    name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2019-09-18 12:42:24,712 :    name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "2019-09-18 12:42:24,713 :    name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "2019-09-18 12:42:24,714 :    name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,715 :    name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,716 :    name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,717 :    name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,718 :    name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,720 :    name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,721 :    name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,722 :    name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,723 :    name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,723 :    name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,725 :    name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,726 :    name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,727 :    name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,728 :    name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "2019-09-18 12:42:24,729 :    name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "2019-09-18 12:42:24,730 :    name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "2019-09-18 12:42:24,731 :    name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,732 :    name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,733 :    name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,735 :    name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,735 :    name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,737 :    name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "2019-09-18 12:42:24,738 :    name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "2019-09-18 12:42:24,739 :    name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "2019-09-18 12:42:24,740 :    name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "2019-09-18 12:42:24,741 :    name = cls/predictions/output_bias:0, shape = (32000,)\n",
            "2019-09-18 12:42:24,742 :    name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "2019-09-18 12:42:24,744 :    name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "2019-09-18 12:42:24,745 :  From bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "2019-09-18 12:42:24,748 :  From bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "2019-09-18 12:42:24,764 :  From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "2019-09-18 12:42:25,402 :  From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2019-09-18 12:42:39,563 :  Create CheckpointSaverHook.\n",
            "2019-09-18 12:42:39,944 :  Done calling model_fn.\n",
            "2019-09-18 12:42:45,324 :  TPU job name worker\n",
            "2019-09-18 12:42:47,087 :  Graph was finalized.\n",
            "2019-09-18 12:42:47,209 :  From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-09-18 12:42:47,346 :  Error recorded from training_loop: The passed save_path is not a valid checkpoint: gs://bert_resourses/bert_model/model.ckpt-102500\n",
            "2019-09-18 12:42:47,347 :  training_loop marked as finished\n",
            "2019-09-18 12:42:47,352 :  Reraising captured error\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-fc1a0ca3c649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2875\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2876\u001b[0;31m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2878\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py\u001b[0m in \u001b[0;36mraise_errors\u001b[0;34m(self, timeout_sec)\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reraising captured error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkept_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m   2869\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2870\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2871\u001b[0;31m           saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m   2872\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2873\u001b[0m       \u001b[0mrendezvous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_loop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1156\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1190\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1191\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \u001b[0msave_summaries_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_summary_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m         log_step_count_steps=log_step_count_steps) as mon_sess:\n\u001b[0m\u001b[1;32m   1481\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mMonitoredTrainingSession\u001b[0;34m(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\u001b[0m\n\u001b[1;32m    582\u001b[0m       \u001b[0msession_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m       stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0mshould_recover\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session_creator, hooks, should_recover, stop_grace_period_secs)\u001b[0m\n\u001b[1;32m    723\u001b[0m         stop_grace_period_secs=stop_grace_period_secs)\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_recover\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RecoverableSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinated_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess_creator)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \"\"\"\n\u001b[1;32m   1199\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m     \u001b[0m_WrappedSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36m_create_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         logging.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;34m\"\"\"Creates a coordinated session.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;31m# Keep the tf_sess for unit testing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m       \u001b[0;31m# We don't want coordinator to suppress any exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_stop_exception_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mcreate_session\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0minit_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0minit_feed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaffold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_feed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         init_fn=self._scaffold.init_fn)\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36mprepare_session\u001b[0;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         config=config)\n\u001b[0m\u001b[1;32m    291\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loaded_from_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minit_fn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_init_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self, master, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;31m# Loads the checkpoint.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecover_last_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_model_checkpoint_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \" +\n\u001b[0;32m-> 1278\u001b[0;31m                        compat.as_text(save_path))\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The passed save_path is not a valid checkpoint: gs://bert_resourses/bert_model/model.ckpt-102500"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_OeXod-fHMT"
      },
      "source": [
        "Training the model with the default parameters for 1 million steps will take ~53 hours. \n",
        "\n",
        "In case the kernel is restarted, you may always continue training from the latest checkpoint. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ZIAAATfzdF"
      },
      "source": [
        "This concludes the guide to pre-training BERT from scratch on a cloud TPU. However, the really fun stuff is still  to come, so stay tuned.\n",
        "\n",
        "Keep learning!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiFH_9Lbze5f"
      },
      "source": [
        "!gsutil cp gs://hermes_assets/russian_uncased_L-12_H-768_A-12.zip gs://bert_resourses/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP7_2pKWzfiZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}