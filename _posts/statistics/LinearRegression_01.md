# 0. 기초

우선적으로 큰 줄기를 잡아보자.

선형회귀의 '선형'은 변수 x에 대한 선형성이 아닌, 모수 b에 대한 선형성 즉, parameter에 대한 선형성이 갖추어진 회귀식을 선형회귀라한다.


가령, 
>y = b1*x^2 + b2*x + b0
위의 경우 x에 대해서는 선형식이 아니나, parameter에 대해서는 선형이다.

>y= b1*b2*x1 + b0
위의 경우 x에 대해서는 선형식이나, parameter에 대해서는 선형이 아니다.


## LinearRegression의 종류
그렇다면 통계방법론의 강의에서 다루는 LinearRegression의 갈래는 다음과 같다.
1. 모수가 2개인 단순 선형회귀 
> y = b1*x + b0
2. 모수가 3개인 다중회귀 (Multivariate Regression)
> y = b1*x1 + b2*x2 + b0
3. 모수가 3개인 다항회귀 (Polynomial Regression) -2의 한 예시이기도하다
 > y = b1*x1^2 + b2*x1 + b0
 
 
 
## Least Squares Method
회귀중 아주아주 보편적이고 유명한 방법은 아무래도 
최소제곱법(최소 자승법, 최소제곱 근사법, 최소자승 근사법이라고도 불린다, least squares method)이 아닐까 싶다.

논의에 앞서, 선형회귀에 대한 전체적인 맥락을 설명해야한다.
선형회귀란, 주어진 데이터에 가장 근사한 함수를 추정하고, 이를 토대로 새로운 데이터의 y값을 예측하고자 하는 통계방법이다.

결국 함수 추정이라는 얘기이다.

최소제곱법은 이러한 함수 추정 방법의 한 갈래인데, 오차항(errors)를 제곱하여 최소가 되도록 함수를 조정하여 만들어주는 것이다.
>cf) 여기서 제곱을 하는 이유는 오차의 부호를 반영하기 위함이다. 예를 들어, 오차가 -1,1 두개가 나올 경우, 실제로는 오차가 있으나, 단순합을하면
>    0이 되므로 오차가 존재하지 않는 것으로 판단한다.

따라서, 오차가 최소가 되도록하는 것이 선형회귀의 핵심이라 할 수 있다.


### 대략적인 내용을 잡아보았당..
다음 내용부터는 난이도가 폭등하니 어려움주의!
