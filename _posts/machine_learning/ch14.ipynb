{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_learning_ch14.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFH6WiaLmG3slOLcmtdFKu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverstar0727/1day-1commit-challenge/blob/master/_posts/machine_learning/ch14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM7geSs5wVkB",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow\n",
        "### Rank & Tensor\n",
        "텐서는 데이터를 담고 있는 다차원배열\n",
        "\n",
        "랭크는 텐서의 차원을 의미함(정수나 실수같은 스칼라는 랭크가 0인 텐서, 벡터는 랭크1인 텐서, 행렬은 랭크2인 텐서)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7meVa-pwyle",
        "colab_type": "text"
      },
      "source": [
        "### 텐서의 랭크와 크기를 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4ZBkLfxayb1",
        "colab_type": "code",
        "outputId": "44cd7c0c-095d-475e-dcf4-1e10682d8128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpwyAIpCxCNd",
        "colab_type": "code",
        "outputId": "a0bd0a9c-146d-4bc8-fde9-cd5618c76fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 텐서 정의\n",
        "t1 = tf.constant(np.pi)\n",
        "t2 = tf.constant([1,2,3,4])\n",
        "t3 = tf.constant([[1,2], [3,4]])\n",
        "\n",
        "# 랭크 확인\n",
        "r1 = tf.rank(t1)\n",
        "r2 = tf.rank(t2)\n",
        "r3 = tf.rank(t3)\n",
        "\n",
        "# 크기 확인\n",
        "s1 = t1.get_shape()\n",
        "s2 = t2.get_shape()\n",
        "s3 = t3.get_shape()\n",
        "\n",
        "print('크기:', s1, s2, s3)\n",
        "print('랭크:', r1.numpy(), r2.numpy(), r3.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "크기: () (4,) (2, 2)\n",
            "랭크: 0 1 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYDgTiZexoWZ",
        "colab_type": "text"
      },
      "source": [
        "## 텐서를 다차원 배열로의 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSsdXAhkxgC7",
        "colab_type": "code",
        "outputId": "342d2329-8c7f-46c5-e1b9-c020ddb3e076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "arr = np.array([[1., 2., 3., 3.5],\n",
        "                [4., 5., 6., 6.5],\n",
        "                [7., 8., 9., 9.5]])\n",
        "\n",
        "T1 = tf.constant(arr)\n",
        "print(T1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1.  2.  3.  3.5]\n",
            " [4.  5.  6.  6.5]\n",
            " [7.  8.  9.  9.5]], shape=(3, 4), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ok92F7tAptL",
        "colab_type": "code",
        "outputId": "cbeb84e0-9eb7-4264-fda6-a5f21524d72b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "s = T1.get_shape()\n",
        "\n",
        "print('T1의 크기:', s)\n",
        "print('T1의 크기:', T1.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T1의 크기: (3, 4)\n",
            "T1의 크기: (3, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CCk1npByM_p",
        "colab_type": "code",
        "outputId": "5b878c52-b516-4ff0-d21a-1b364757dfc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "T2 = tf.Variable(np.random.normal(size = s))\n",
        "print(T2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float64, numpy=\n",
            "array([[-1.63246222, -0.42681701,  0.71522782, -0.84449521],\n",
            "       [ 0.90108304,  1.12569306,  0.77601897,  0.63955153],\n",
            "       [-0.32781222,  1.69501443,  0.31533969,  0.29962497]])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv5G-n3cyTbM",
        "colab_type": "code",
        "outputId": "1c3256dd-5e31-4464-973c-a3a04e41dcaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "T3 = tf.Variable(np.random.normal(size = s[0]))\n",
        "print(T3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=(3,) dtype=float64, numpy=array([-1.48185974, -0.67685158, -2.38403235])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vw7OWq8AuQu",
        "colab_type": "code",
        "outputId": "0d03c95e-0cab-4c2e-95fc-68ae1355a6ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 텐서의 크기 변환 tf.reshape이용\n",
        "# shape중 하나에 -1을 사용할 수 있음()\n",
        "T4 = tf.reshape(T1, shape = [1, 1, -1])\n",
        "print(T4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[[1.  2.  3.  3.5 4.  5.  6.  6.5 7.  8.  9.  9.5]]], shape=(1, 1, 12), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sKNAI_nBIS8",
        "colab_type": "code",
        "outputId": "aacc5452-49cb-4475-f058-fd8f1055a7f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "T5 = tf.reshape(T1, shape = [1, 3, -1])\n",
        "print(T5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[1.  2.  3.  3.5]\n",
            "  [4.  5.  6.  6.5]\n",
            "  [7.  8.  9.  9.5]]], shape=(1, 3, 4), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rgKbpAjDJ37",
        "colab_type": "code",
        "outputId": "c370e0ca-671a-4519-b9b7-8dd9521fb5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "# 전치\n",
        "# perm으로 차원지정\n",
        "T6 = tf.transpose(T5, perm = [2, 1, 0])\n",
        "print(T6)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[1. ]\n",
            "  [4. ]\n",
            "  [7. ]]\n",
            "\n",
            " [[2. ]\n",
            "  [5. ]\n",
            "  [8. ]]\n",
            "\n",
            " [[3. ]\n",
            "  [6. ]\n",
            "  [9. ]]\n",
            "\n",
            " [[3.5]\n",
            "  [6.5]\n",
            "  [9.5]]], shape=(4, 3, 1), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca5nc0n8E2DP",
        "colab_type": "code",
        "outputId": "bbcc843d-b527-4249-9fdc-fc5727187aa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "T7 = tf.transpose(T5, perm = [0, 2, 1])\n",
        "print(T7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[1.  4.  7. ]\n",
            "  [2.  5.  8. ]\n",
            "  [3.  6.  9. ]\n",
            "  [3.5 6.5 9.5]]], shape=(1, 4, 3), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp4mdpZEE9xj",
        "colab_type": "code",
        "outputId": "96a0bcaf-2202-44d7-8e9c-a7a7a165fdb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# 텐서 분할\n",
        "t5_split = tf.split(T5, num_or_size_splits = 2, axis = 2)\n",
        "print(t5_split)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(1, 3, 2), dtype=float64, numpy=\n",
            "array([[[1., 2.],\n",
            "        [4., 5.],\n",
            "        [7., 8.]]])>, <tf.Tensor: shape=(1, 3, 2), dtype=float64, numpy=\n",
            "array([[[3. , 3.5],\n",
            "        [6. , 6.5],\n",
            "        [9. , 9.5]]])>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T45XFGaLFMsL",
        "colab_type": "code",
        "outputId": "22e85f2f-ae7c-4c5a-e67b-9704b8eae629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# concat함수를 이용한 같은 dtype 텐서 합성\n",
        "# t1,t2의 숫자를 바꿔가며 연습해볼 것\n",
        "t1 = tf.ones(shape = (5, 1), dtype = tf.float32)\n",
        "t2 = tf.zeros(shape = (5, 1), dtype = tf.float32)\n",
        "\n",
        "print(t1)\n",
        "print(t2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]], shape=(5, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]], shape=(5, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epylT42yFgmY",
        "colab_type": "code",
        "outputId": "a5ef70db-3d32-4de3-f690-af7d20747c33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "t3 = tf.concat([t1, t2], axis = 0)\n",
        "print(t3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]], shape=(10, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgogoRuiFmGy",
        "colab_type": "code",
        "outputId": "fecc49a4-eaa5-4285-c6db-5caa19f78103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "t4 = tf.concat([t1,t2], axis = 1)\n",
        "print(t4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]], shape=(5, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S17SguJxFrU5",
        "colab_type": "code",
        "outputId": "e5abb53d-0b72-4fb9-8db9-2efb32894430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# tensorflow 계산\n",
        "# z = 2 * (a - b) + c\n",
        "a = tf.constant(1)\n",
        "b = tf.constant(2)\n",
        "c = tf.constant(3)\n",
        "\n",
        "z = 2 * (a - b) + c\n",
        "\n",
        "print(z.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds58QgGeczCe",
        "colab_type": "text"
      },
      "source": [
        "tensorflow 2.x에서는 tf.function데코레이터를 사용하여 일반 파이썬 함수를 호출가능한 그래프 객체로 만들어 놓는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqiIoqZ8NKec",
        "colab_type": "code",
        "outputId": "711468a3-a0e4-4d10-9200-cb910fac9017",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "@tf.function\n",
        "def simple_func():\n",
        "  a = tf.constant(1)\n",
        "  b = tf.constant(2)\n",
        "  c = tf.constant(3)\n",
        "\n",
        "  z = 2 * (a - b) + c\n",
        "  \n",
        "  return z\n",
        "\n",
        "print(simple_func().numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm_njhBQSaDR",
        "colab_type": "code",
        "outputId": "f7d54b83-6e91-4a5a-a943-3452eaa297b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 데코레이터에 의해 객체가 바뀌었음을 확인\n",
        "print(simple_func.__class__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.eager.def_function.Function'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyFY7On9TPSb",
        "colab_type": "code",
        "outputId": "83a20ae1-e40f-4287-dc0f-72b5979051bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 다른방식으로 표현\n",
        "# 데코레이터가 아닌 재정의를 통한 방법\n",
        "def simple_func():\n",
        "  a = tf.constant(1, name = 'a')\n",
        "  b = tf.constant(2, name = 'b')\n",
        "  c = tf.constant(3, name = 'c')\n",
        "\n",
        "  z = 2 * (a - b) + c\n",
        "\n",
        "  return z\n",
        "\n",
        "simple_func = tf.function(simple_func)\n",
        "print(simple_func().numpy())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hAF12jLhwrm",
        "colab_type": "code",
        "outputId": "7bfb2606-835c-4e84-ec05-ec3269cb676a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "con_func = simple_func.get_concrete_function()\n",
        "con_func.graph.get_operations()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Operation 'a' type=Const>,\n",
              " <tf.Operation 'b' type=Const>,\n",
              " <tf.Operation 'c' type=Const>,\n",
              " <tf.Operation 'sub' type=Sub>,\n",
              " <tf.Operation 'mul/x' type=Const>,\n",
              " <tf.Operation 'mul' type=Mul>,\n",
              " <tf.Operation 'add' type=AddV2>,\n",
              " <tf.Operation 'Identity' type=Identity>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVuPoTwFiCkv",
        "colab_type": "code",
        "outputId": "0dc8795f-f092-455c-8fc8-cb08c305863f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 그래프 정의\n",
        "con_func.graph.as_graph_def()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "node {\n",
              "  name: \"a\"\n",
              "  op: \"Const\"\n",
              "  attr {\n",
              "    key: \"dtype\"\n",
              "    value {\n",
              "      type: DT_INT32\n",
              "    }\n",
              "  }\n",
              "  attr {\n",
              "    key: \"value\"\n",
              "    value {\n",
              "      tensor {\n",
              "        dtype: DT_INT32\n",
              "        tensor_shape {\n",
              "        }\n",
              "        int_val: 1\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "}\n",
              "node {\n",
              "  name: \"b\"\n",
              "  op: \"Const\"\n",
              "  attr {\n",
              "    key: \"dtype\"\n",
              "    value {\n",
              "      type: DT_INT32\n",
              "    }\n",
              "  }\n",
              "  attr {\n",
              "    key: \"value\"\n",
              "    value {\n",
              "      tensor {\n",
              "        dtype: DT_INT32\n",
              "        tensor_shape {\n",
              "        }\n",
              "        int_val: 2\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "}\n",
              "node {\n",
              "  name: \"c\"\n",
              "  op: \"Const\"\n",
              "  attr {\n",
              "    key: \"dtype\"\n",
              "    value {\n",
              "      type: DT_INT32\n",
              "    }\n",
              "  }\n",
              "  attr {\n",
              "    key: \"value\"\n",
              "    value {\n",
              "      tensor {\n",
              "        dtype: DT_INT32\n",
              "        tensor_shape {\n",
              "        }\n",
              "        int_val: 3\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "}\n",
              "node {\n",
              "  name: \"sub\"\n",
              "  op: \"Sub\"\n",
              "  input: \"a\"\n",
              "  input: \"b\"\n",
              "  attr {\n",
              "    key: \"T\"\n",
              "    value {\n",
              "      type: DT_INT32\n",
              "    }\n",
              "  }\n",
              "}\n",
              "node {\n",
              "  name: \"mul/x\"\n",
              "  op: \"Const\"\n",
              "  attr {\n",
              "    key: \"dtype\"\n",
              "    value {\n",
              "      type: DT_INT32\n",
              "    }\n",
              "  }\n",
              "  attr {\n",
              "    key: \"value\"\n",
              "    value {\n",
              "      tensor {\n",
              "        dtype: DT_INT32\n",
              "        tensor_shape {\n",
              "        }\n",
              "        int_val: 2\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "}\n",
              "node {\n",
              "  name: \"mul\"\n",
              "  op: \"Mul\"\n",
              "  input: \"mul/x\"\n",
              "  input: \"sub\"\n",
              "  attr {\n",
              "    key: \"T\"\n",
              "    value {\n",
              "      type: DT_INT32\n",
              "    }\n",
              "  }\n",
              "}\n",
              "node {\n",
              "  name: \"add\"\n",
              "  op: \"AddV2\"\n",
              "  input: \"mul\"\n",
              "  input: \"c\"\n",
              "  attr {\n",
              "    key: \"T\"\n",
              "    value {\n",
              "      type: DT_INT32\n",
              "    }\n",
              "  }\n",
              "}\n",
              "node {\n",
              "  name: \"Identity\"\n",
              "  op: \"Identity\"\n",
              "  input: \"add\"\n",
              "  attr {\n",
              "    key: \"T\"\n",
              "    value {\n",
              "      type: DT_INT32\n",
              "    }\n",
              "  }\n",
              "}\n",
              "versions {\n",
              "  producer: 175\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0AX4Seqi_an",
        "colab_type": "text"
      },
      "source": [
        "## tensorflow의 변수\n",
        "https://www.tensorflow.org/guide/variables\n",
        "참고\n",
        "\n",
        "변수 정의: tf.Variable((initial_value), name = (optional_name))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geHs51Yqjn5K",
        "colab_type": "code",
        "outputId": "6b90e571-6919-4d46-e0fc-aec935a7635f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "w2 = tf.Variable(np.array([[1,2,3,4], [2,3,4,5]]))\n",
        "print(w2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=(2, 4) dtype=int64, numpy=\n",
            "array([[1, 2, 3, 4],\n",
            "       [2, 3, 4, 5]])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYx8XdSHkg9O",
        "colab_type": "code",
        "outputId": "be5d63cf-3eff-450a-e98f-aa2c8457fe08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(w2 + 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[2 3 4 5]\n",
            " [3 4 5 6]], shape=(2, 4), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBg-KK4XkmIK",
        "colab_type": "code",
        "outputId": "f9efacad-44a5-4083-f179-cbbd002a8392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# assign 메소드 이용\n",
        "w2.assign(w2 + 1)\n",
        "print(w2.numpy())\n",
        "# 바로적용되는 것을 확인 할 수 있음"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 3 4 5]\n",
            " [3 4 5 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAuI7MI0lDDG",
        "colab_type": "text"
      },
      "source": [
        "## keras API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa6DrXcdlE9H",
        "colab_type": "text"
      },
      "source": [
        "### Sequential model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLYiIL1Ak3dR",
        "colab_type": "code",
        "outputId": "eb23d42f-1f6a-4bad-efa5-fe01c5ba604f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# 간단한 회귀용 데이터 생성\n",
        "# 샘플개수 200개\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "def make_random_data():\n",
        "  x = np.random.uniform(low = -2, high = 2, size = 200)\n",
        "  y = []\n",
        "  for t in x:\n",
        "    r = np.random.normal(loc = 0.0, \n",
        "                         scale = (0.5 + t*t/3),\n",
        "                         size = None)\n",
        "    y.append(r)\n",
        "  return x, 1.726 * x - 0.84 + np.array(y)\n",
        "\n",
        "x, y = make_random_data()\n",
        "\n",
        "plt.plot(x, y, 'o')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoYGup7slzeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 훈련 150ro 테스트 50개로 split\n",
        "x_train, y_train = x[:150], y[:150]\n",
        "x_test, y_test = x[150:], y[:150]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSgj3YbkmKyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 생성\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 1, input_dim = 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTJzloK0mThB",
        "colab_type": "code",
        "outputId": "2d7c45c9-b3b6-43e9-9576-0eae1998ad46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 1)                 2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgKS5hb2mWe8",
        "colab_type": "code",
        "outputId": "7fba5c2f-7103-4da5-a9db-154100b7db27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 모델 컴파일 \n",
        "# 경사하강법 옵티마이저 = SGD, 손실함수 = MSE\n",
        "model.compile(optimizer = 'sgd', loss = 'mse')\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(x_train, y_train, epochs = 500, validation_split = 0.3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 45 samples\n",
            "Epoch 1/500\n",
            "105/105 [==============================] - 0s 2ms/sample - loss: 1.6696 - val_loss: 1.3776\n",
            "Epoch 2/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 1.5248 - val_loss: 1.2908\n",
            "Epoch 3/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 1.4055 - val_loss: 1.2101\n",
            "Epoch 4/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 1.3062 - val_loss: 1.1436\n",
            "Epoch 5/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 1.2200 - val_loss: 1.0875\n",
            "Epoch 6/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 1.1502 - val_loss: 1.0433\n",
            "Epoch 7/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 1.0893 - val_loss: 1.0002\n",
            "Epoch 8/500\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 1.0287 - val_loss: 0.9735\n",
            "Epoch 9/500\n",
            "105/105 [==============================] - 0s 235us/sample - loss: 0.9896 - val_loss: 0.9487\n",
            "Epoch 10/500\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.9528 - val_loss: 0.9305\n",
            "Epoch 11/500\n",
            "105/105 [==============================] - 0s 186us/sample - loss: 0.9255 - val_loss: 0.9199\n",
            "Epoch 12/500\n",
            "105/105 [==============================] - 0s 192us/sample - loss: 0.9079 - val_loss: 0.9098\n",
            "Epoch 13/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.8888 - val_loss: 0.9008\n",
            "Epoch 14/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.8729 - val_loss: 0.8896\n",
            "Epoch 15/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.8534 - val_loss: 0.8839\n",
            "Epoch 16/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.8388 - val_loss: 0.8779\n",
            "Epoch 17/500\n",
            "105/105 [==============================] - 0s 168us/sample - loss: 0.8254 - val_loss: 0.8740\n",
            "Epoch 18/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.8134 - val_loss: 0.8715\n",
            "Epoch 19/500\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.8007 - val_loss: 0.8704\n",
            "Epoch 20/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7945 - val_loss: 0.8693\n",
            "Epoch 21/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7901 - val_loss: 0.8702\n",
            "Epoch 22/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7849 - val_loss: 0.8688\n",
            "Epoch 23/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7807 - val_loss: 0.8702\n",
            "Epoch 24/500\n",
            "105/105 [==============================] - 0s 189us/sample - loss: 0.7767 - val_loss: 0.8717\n",
            "Epoch 25/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7769 - val_loss: 0.8734\n",
            "Epoch 26/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7748 - val_loss: 0.8727\n",
            "Epoch 27/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7717 - val_loss: 0.8748\n",
            "Epoch 28/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7692 - val_loss: 0.8742\n",
            "Epoch 29/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7672 - val_loss: 0.8768\n",
            "Epoch 30/500\n",
            "105/105 [==============================] - 0s 168us/sample - loss: 0.7648 - val_loss: 0.8798\n",
            "Epoch 31/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7635 - val_loss: 0.8773\n",
            "Epoch 32/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7622 - val_loss: 0.8800\n",
            "Epoch 33/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7606 - val_loss: 0.8807\n",
            "Epoch 34/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7606 - val_loss: 0.8826\n",
            "Epoch 35/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7613 - val_loss: 0.8843\n",
            "Epoch 36/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7607 - val_loss: 0.8861\n",
            "Epoch 37/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7606 - val_loss: 0.8883\n",
            "Epoch 38/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7598 - val_loss: 0.8886\n",
            "Epoch 39/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7597 - val_loss: 0.8893\n",
            "Epoch 40/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7582 - val_loss: 0.8884\n",
            "Epoch 41/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7589 - val_loss: 0.8858\n",
            "Epoch 42/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7584 - val_loss: 0.8845\n",
            "Epoch 43/500\n",
            "105/105 [==============================] - 0s 121us/sample - loss: 0.7598 - val_loss: 0.8857\n",
            "Epoch 44/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7585 - val_loss: 0.8832\n",
            "Epoch 45/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7589 - val_loss: 0.8895\n",
            "Epoch 46/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7575 - val_loss: 0.8898\n",
            "Epoch 47/500\n",
            "105/105 [==============================] - 0s 208us/sample - loss: 0.7585 - val_loss: 0.8914\n",
            "Epoch 48/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7569 - val_loss: 0.8909\n",
            "Epoch 49/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7571 - val_loss: 0.8911\n",
            "Epoch 50/500\n",
            "105/105 [==============================] - 0s 182us/sample - loss: 0.7574 - val_loss: 0.8971\n",
            "Epoch 51/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7577 - val_loss: 0.9018\n",
            "Epoch 52/500\n",
            "105/105 [==============================] - 0s 183us/sample - loss: 0.7576 - val_loss: 0.8982\n",
            "Epoch 53/500\n",
            "105/105 [==============================] - 0s 182us/sample - loss: 0.7594 - val_loss: 0.8974\n",
            "Epoch 54/500\n",
            "105/105 [==============================] - 0s 188us/sample - loss: 0.7564 - val_loss: 0.8975\n",
            "Epoch 55/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7576 - val_loss: 0.8968\n",
            "Epoch 56/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7572 - val_loss: 0.8991\n",
            "Epoch 57/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7564 - val_loss: 0.9048\n",
            "Epoch 58/500\n",
            "105/105 [==============================] - 0s 169us/sample - loss: 0.7558 - val_loss: 0.9052\n",
            "Epoch 59/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7559 - val_loss: 0.9068\n",
            "Epoch 60/500\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.7566 - val_loss: 0.9066\n",
            "Epoch 61/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7578 - val_loss: 0.9087\n",
            "Epoch 62/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7567 - val_loss: 0.9168\n",
            "Epoch 63/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7567 - val_loss: 0.9150\n",
            "Epoch 64/500\n",
            "105/105 [==============================] - 0s 131us/sample - loss: 0.7562 - val_loss: 0.9156\n",
            "Epoch 65/500\n",
            "105/105 [==============================] - 0s 181us/sample - loss: 0.7575 - val_loss: 0.9145\n",
            "Epoch 66/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7574 - val_loss: 0.9211\n",
            "Epoch 67/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7580 - val_loss: 0.9187\n",
            "Epoch 68/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7582 - val_loss: 0.9222\n",
            "Epoch 69/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7590 - val_loss: 0.9223\n",
            "Epoch 70/500\n",
            "105/105 [==============================] - 0s 186us/sample - loss: 0.7602 - val_loss: 0.9188\n",
            "Epoch 71/500\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 0.7573 - val_loss: 0.9170\n",
            "Epoch 72/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7586 - val_loss: 0.9166\n",
            "Epoch 73/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7593 - val_loss: 0.9197\n",
            "Epoch 74/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7586 - val_loss: 0.9144\n",
            "Epoch 75/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7573 - val_loss: 0.9167\n",
            "Epoch 76/500\n",
            "105/105 [==============================] - 0s 135us/sample - loss: 0.7571 - val_loss: 0.9166\n",
            "Epoch 77/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7570 - val_loss: 0.9179\n",
            "Epoch 78/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7580 - val_loss: 0.9172\n",
            "Epoch 79/500\n",
            "105/105 [==============================] - 0s 181us/sample - loss: 0.7588 - val_loss: 0.9161\n",
            "Epoch 80/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7579 - val_loss: 0.9121\n",
            "Epoch 81/500\n",
            "105/105 [==============================] - 0s 175us/sample - loss: 0.7567 - val_loss: 0.9116\n",
            "Epoch 82/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7575 - val_loss: 0.9061\n",
            "Epoch 83/500\n",
            "105/105 [==============================] - 0s 185us/sample - loss: 0.7558 - val_loss: 0.9059\n",
            "Epoch 84/500\n",
            "105/105 [==============================] - 0s 134us/sample - loss: 0.7559 - val_loss: 0.9062\n",
            "Epoch 85/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7560 - val_loss: 0.9068\n",
            "Epoch 86/500\n",
            "105/105 [==============================] - 0s 125us/sample - loss: 0.7560 - val_loss: 0.9064\n",
            "Epoch 87/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7576 - val_loss: 0.9093\n",
            "Epoch 88/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7562 - val_loss: 0.9082\n",
            "Epoch 89/500\n",
            "105/105 [==============================] - 0s 198us/sample - loss: 0.7561 - val_loss: 0.9066\n",
            "Epoch 90/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7566 - val_loss: 0.9078\n",
            "Epoch 91/500\n",
            "105/105 [==============================] - 0s 225us/sample - loss: 0.7558 - val_loss: 0.9113\n",
            "Epoch 92/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7563 - val_loss: 0.9090\n",
            "Epoch 93/500\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.7562 - val_loss: 0.9087\n",
            "Epoch 94/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7564 - val_loss: 0.9084\n",
            "Epoch 95/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7565 - val_loss: 0.9070\n",
            "Epoch 96/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7593 - val_loss: 0.9067\n",
            "Epoch 97/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7555 - val_loss: 0.9059\n",
            "Epoch 98/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7560 - val_loss: 0.9046\n",
            "Epoch 99/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7557 - val_loss: 0.9079\n",
            "Epoch 100/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7579 - val_loss: 0.9071\n",
            "Epoch 101/500\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.7571 - val_loss: 0.9077\n",
            "Epoch 102/500\n",
            "105/105 [==============================] - 0s 186us/sample - loss: 0.7571 - val_loss: 0.9069\n",
            "Epoch 103/500\n",
            "105/105 [==============================] - 0s 185us/sample - loss: 0.7565 - val_loss: 0.9127\n",
            "Epoch 104/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7558 - val_loss: 0.9119\n",
            "Epoch 105/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7575 - val_loss: 0.9068\n",
            "Epoch 106/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7568 - val_loss: 0.9091\n",
            "Epoch 107/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7567 - val_loss: 0.9104\n",
            "Epoch 108/500\n",
            "105/105 [==============================] - 0s 214us/sample - loss: 0.7573 - val_loss: 0.9154\n",
            "Epoch 109/500\n",
            "105/105 [==============================] - 0s 202us/sample - loss: 0.7571 - val_loss: 0.9114\n",
            "Epoch 110/500\n",
            "105/105 [==============================] - 0s 189us/sample - loss: 0.7564 - val_loss: 0.9165\n",
            "Epoch 111/500\n",
            "105/105 [==============================] - 0s 190us/sample - loss: 0.7569 - val_loss: 0.9127\n",
            "Epoch 112/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7566 - val_loss: 0.9129\n",
            "Epoch 113/500\n",
            "105/105 [==============================] - 0s 180us/sample - loss: 0.7559 - val_loss: 0.9132\n",
            "Epoch 114/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7569 - val_loss: 0.9144\n",
            "Epoch 115/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7562 - val_loss: 0.9142\n",
            "Epoch 116/500\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.7561 - val_loss: 0.9119\n",
            "Epoch 117/500\n",
            "105/105 [==============================] - 0s 180us/sample - loss: 0.7578 - val_loss: 0.9076\n",
            "Epoch 118/500\n",
            "105/105 [==============================] - 0s 196us/sample - loss: 0.7575 - val_loss: 0.9031\n",
            "Epoch 119/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7565 - val_loss: 0.9037\n",
            "Epoch 120/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7570 - val_loss: 0.9021\n",
            "Epoch 121/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7565 - val_loss: 0.8999\n",
            "Epoch 122/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7558 - val_loss: 0.8994\n",
            "Epoch 123/500\n",
            "105/105 [==============================] - 0s 244us/sample - loss: 0.7558 - val_loss: 0.8986\n",
            "Epoch 124/500\n",
            "105/105 [==============================] - 0s 132us/sample - loss: 0.7573 - val_loss: 0.9039\n",
            "Epoch 125/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7562 - val_loss: 0.9086\n",
            "Epoch 126/500\n",
            "105/105 [==============================] - 0s 134us/sample - loss: 0.7567 - val_loss: 0.9065\n",
            "Epoch 127/500\n",
            "105/105 [==============================] - 0s 129us/sample - loss: 0.7557 - val_loss: 0.9052\n",
            "Epoch 128/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7567 - val_loss: 0.9042\n",
            "Epoch 129/500\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.7565 - val_loss: 0.9078\n",
            "Epoch 130/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7569 - val_loss: 0.9065\n",
            "Epoch 131/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7557 - val_loss: 0.9047\n",
            "Epoch 132/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7565 - val_loss: 0.9003\n",
            "Epoch 133/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7560 - val_loss: 0.9020\n",
            "Epoch 134/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7562 - val_loss: 0.8989\n",
            "Epoch 135/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7563 - val_loss: 0.8998\n",
            "Epoch 136/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7569 - val_loss: 0.9025\n",
            "Epoch 137/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7571 - val_loss: 0.9000\n",
            "Epoch 138/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7561 - val_loss: 0.8992\n",
            "Epoch 139/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7593 - val_loss: 0.8982\n",
            "Epoch 140/500\n",
            "105/105 [==============================] - 0s 184us/sample - loss: 0.7579 - val_loss: 0.8990\n",
            "Epoch 141/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7562 - val_loss: 0.8982\n",
            "Epoch 142/500\n",
            "105/105 [==============================] - 0s 168us/sample - loss: 0.7595 - val_loss: 0.8965\n",
            "Epoch 143/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7568 - val_loss: 0.8963\n",
            "Epoch 144/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7580 - val_loss: 0.8919\n",
            "Epoch 145/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7572 - val_loss: 0.8960\n",
            "Epoch 146/500\n",
            "105/105 [==============================] - 0s 132us/sample - loss: 0.7573 - val_loss: 0.8949\n",
            "Epoch 147/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7564 - val_loss: 0.8943\n",
            "Epoch 148/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7563 - val_loss: 0.8956\n",
            "Epoch 149/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7580 - val_loss: 0.8957\n",
            "Epoch 150/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7571 - val_loss: 0.9009\n",
            "Epoch 151/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7559 - val_loss: 0.8999\n",
            "Epoch 152/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7573 - val_loss: 0.9010\n",
            "Epoch 153/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7562 - val_loss: 0.9007\n",
            "Epoch 154/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7567 - val_loss: 0.8992\n",
            "Epoch 155/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7569 - val_loss: 0.8998\n",
            "Epoch 156/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7563 - val_loss: 0.9007\n",
            "Epoch 157/500\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.7567 - val_loss: 0.9023\n",
            "Epoch 158/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7570 - val_loss: 0.9017\n",
            "Epoch 159/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7572 - val_loss: 0.9009\n",
            "Epoch 160/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7564 - val_loss: 0.9040\n",
            "Epoch 161/500\n",
            "105/105 [==============================] - 0s 169us/sample - loss: 0.7563 - val_loss: 0.9033\n",
            "Epoch 162/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7571 - val_loss: 0.9050\n",
            "Epoch 163/500\n",
            "105/105 [==============================] - 0s 188us/sample - loss: 0.7564 - val_loss: 0.9051\n",
            "Epoch 164/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7567 - val_loss: 0.9060\n",
            "Epoch 165/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7565 - val_loss: 0.9047\n",
            "Epoch 166/500\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.7560 - val_loss: 0.9039\n",
            "Epoch 167/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7579 - val_loss: 0.9043\n",
            "Epoch 168/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7565 - val_loss: 0.9045\n",
            "Epoch 169/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7566 - val_loss: 0.9019\n",
            "Epoch 170/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7564 - val_loss: 0.9019\n",
            "Epoch 171/500\n",
            "105/105 [==============================] - 0s 196us/sample - loss: 0.7562 - val_loss: 0.9043\n",
            "Epoch 172/500\n",
            "105/105 [==============================] - 0s 230us/sample - loss: 0.7578 - val_loss: 0.9047\n",
            "Epoch 173/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7564 - val_loss: 0.9049\n",
            "Epoch 174/500\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.7563 - val_loss: 0.9041\n",
            "Epoch 175/500\n",
            "105/105 [==============================] - 0s 135us/sample - loss: 0.7558 - val_loss: 0.9034\n",
            "Epoch 176/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7564 - val_loss: 0.8996\n",
            "Epoch 177/500\n",
            "105/105 [==============================] - 0s 130us/sample - loss: 0.7560 - val_loss: 0.9005\n",
            "Epoch 178/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7568 - val_loss: 0.9018\n",
            "Epoch 179/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7583 - val_loss: 0.9061\n",
            "Epoch 180/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7566 - val_loss: 0.9056\n",
            "Epoch 181/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7559 - val_loss: 0.9055\n",
            "Epoch 182/500\n",
            "105/105 [==============================] - 0s 129us/sample - loss: 0.7563 - val_loss: 0.9104\n",
            "Epoch 183/500\n",
            "105/105 [==============================] - 0s 175us/sample - loss: 0.7584 - val_loss: 0.9205\n",
            "Epoch 184/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7573 - val_loss: 0.9186\n",
            "Epoch 185/500\n",
            "105/105 [==============================] - 0s 179us/sample - loss: 0.7579 - val_loss: 0.9206\n",
            "Epoch 186/500\n",
            "105/105 [==============================] - 0s 216us/sample - loss: 0.7577 - val_loss: 0.9253\n",
            "Epoch 187/500\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.7587 - val_loss: 0.9229\n",
            "Epoch 188/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7582 - val_loss: 0.9233\n",
            "Epoch 189/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7572 - val_loss: 0.9218\n",
            "Epoch 190/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7583 - val_loss: 0.9270\n",
            "Epoch 191/500\n",
            "105/105 [==============================] - 0s 190us/sample - loss: 0.7607 - val_loss: 0.9221\n",
            "Epoch 192/500\n",
            "105/105 [==============================] - 0s 232us/sample - loss: 0.7579 - val_loss: 0.9190\n",
            "Epoch 193/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7582 - val_loss: 0.9155\n",
            "Epoch 194/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7574 - val_loss: 0.9164\n",
            "Epoch 195/500\n",
            "105/105 [==============================] - 0s 192us/sample - loss: 0.7570 - val_loss: 0.9154\n",
            "Epoch 196/500\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.7582 - val_loss: 0.9136\n",
            "Epoch 197/500\n",
            "105/105 [==============================] - 0s 169us/sample - loss: 0.7567 - val_loss: 0.9140\n",
            "Epoch 198/500\n",
            "105/105 [==============================] - 0s 195us/sample - loss: 0.7575 - val_loss: 0.9092\n",
            "Epoch 199/500\n",
            "105/105 [==============================] - 0s 242us/sample - loss: 0.7570 - val_loss: 0.9034\n",
            "Epoch 200/500\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 0.7564 - val_loss: 0.9012\n",
            "Epoch 201/500\n",
            "105/105 [==============================] - 0s 239us/sample - loss: 0.7559 - val_loss: 0.9010\n",
            "Epoch 202/500\n",
            "105/105 [==============================] - 0s 192us/sample - loss: 0.7563 - val_loss: 0.8988\n",
            "Epoch 203/500\n",
            "105/105 [==============================] - 0s 185us/sample - loss: 0.7560 - val_loss: 0.8984\n",
            "Epoch 204/500\n",
            "105/105 [==============================] - 0s 188us/sample - loss: 0.7572 - val_loss: 0.8993\n",
            "Epoch 205/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7567 - val_loss: 0.8991\n",
            "Epoch 206/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7564 - val_loss: 0.9060\n",
            "Epoch 207/500\n",
            "105/105 [==============================] - 0s 226us/sample - loss: 0.7568 - val_loss: 0.9082\n",
            "Epoch 208/500\n",
            "105/105 [==============================] - 0s 192us/sample - loss: 0.7564 - val_loss: 0.9103\n",
            "Epoch 209/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7573 - val_loss: 0.9085\n",
            "Epoch 210/500\n",
            "105/105 [==============================] - 0s 178us/sample - loss: 0.7569 - val_loss: 0.9106\n",
            "Epoch 211/500\n",
            "105/105 [==============================] - 0s 212us/sample - loss: 0.7568 - val_loss: 0.9137\n",
            "Epoch 212/500\n",
            "105/105 [==============================] - 0s 207us/sample - loss: 0.7566 - val_loss: 0.9126\n",
            "Epoch 213/500\n",
            "105/105 [==============================] - 0s 195us/sample - loss: 0.7565 - val_loss: 0.9088\n",
            "Epoch 214/500\n",
            "105/105 [==============================] - 0s 198us/sample - loss: 0.7565 - val_loss: 0.9070\n",
            "Epoch 215/500\n",
            "105/105 [==============================] - 0s 180us/sample - loss: 0.7563 - val_loss: 0.9036\n",
            "Epoch 216/500\n",
            "105/105 [==============================] - 0s 192us/sample - loss: 0.7575 - val_loss: 0.9008\n",
            "Epoch 217/500\n",
            "105/105 [==============================] - 0s 200us/sample - loss: 0.7572 - val_loss: 0.9049\n",
            "Epoch 218/500\n",
            "105/105 [==============================] - 0s 178us/sample - loss: 0.7568 - val_loss: 0.9037\n",
            "Epoch 219/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7564 - val_loss: 0.9039\n",
            "Epoch 220/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7566 - val_loss: 0.9081\n",
            "Epoch 221/500\n",
            "105/105 [==============================] - 0s 195us/sample - loss: 0.7563 - val_loss: 0.9087\n",
            "Epoch 222/500\n",
            "105/105 [==============================] - 0s 240us/sample - loss: 0.7560 - val_loss: 0.9073\n",
            "Epoch 223/500\n",
            "105/105 [==============================] - 0s 188us/sample - loss: 0.7560 - val_loss: 0.9049\n",
            "Epoch 224/500\n",
            "105/105 [==============================] - 0s 220us/sample - loss: 0.7559 - val_loss: 0.9042\n",
            "Epoch 225/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7563 - val_loss: 0.9036\n",
            "Epoch 226/500\n",
            "105/105 [==============================] - 0s 175us/sample - loss: 0.7560 - val_loss: 0.9059\n",
            "Epoch 227/500\n",
            "105/105 [==============================] - 0s 191us/sample - loss: 0.7567 - val_loss: 0.9059\n",
            "Epoch 228/500\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.7566 - val_loss: 0.9029\n",
            "Epoch 229/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7560 - val_loss: 0.9008\n",
            "Epoch 230/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7569 - val_loss: 0.9070\n",
            "Epoch 231/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7562 - val_loss: 0.9050\n",
            "Epoch 232/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7559 - val_loss: 0.9072\n",
            "Epoch 233/500\n",
            "105/105 [==============================] - 0s 179us/sample - loss: 0.7571 - val_loss: 0.9058\n",
            "Epoch 234/500\n",
            "105/105 [==============================] - 0s 175us/sample - loss: 0.7584 - val_loss: 0.9051\n",
            "Epoch 235/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7556 - val_loss: 0.9032\n",
            "Epoch 236/500\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.7585 - val_loss: 0.9047\n",
            "Epoch 237/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7560 - val_loss: 0.9040\n",
            "Epoch 238/500\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.7567 - val_loss: 0.9011\n",
            "Epoch 239/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7556 - val_loss: 0.9007\n",
            "Epoch 240/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7556 - val_loss: 0.8992\n",
            "Epoch 241/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7570 - val_loss: 0.8953\n",
            "Epoch 242/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7564 - val_loss: 0.8955\n",
            "Epoch 243/500\n",
            "105/105 [==============================] - 0s 195us/sample - loss: 0.7582 - val_loss: 0.8969\n",
            "Epoch 244/500\n",
            "105/105 [==============================] - 0s 182us/sample - loss: 0.7561 - val_loss: 0.8972\n",
            "Epoch 245/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7564 - val_loss: 0.9035\n",
            "Epoch 246/500\n",
            "105/105 [==============================] - 0s 214us/sample - loss: 0.7563 - val_loss: 0.9020\n",
            "Epoch 247/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7558 - val_loss: 0.9013\n",
            "Epoch 248/500\n",
            "105/105 [==============================] - 0s 222us/sample - loss: 0.7581 - val_loss: 0.8990\n",
            "Epoch 249/500\n",
            "105/105 [==============================] - 0s 173us/sample - loss: 0.7560 - val_loss: 0.8983\n",
            "Epoch 250/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7576 - val_loss: 0.8963\n",
            "Epoch 251/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7582 - val_loss: 0.8970\n",
            "Epoch 252/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7569 - val_loss: 0.8972\n",
            "Epoch 253/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7569 - val_loss: 0.8976\n",
            "Epoch 254/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7565 - val_loss: 0.8982\n",
            "Epoch 255/500\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.7567 - val_loss: 0.8935\n",
            "Epoch 256/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7566 - val_loss: 0.8951\n",
            "Epoch 257/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7571 - val_loss: 0.8963\n",
            "Epoch 258/500\n",
            "105/105 [==============================] - 0s 194us/sample - loss: 0.7570 - val_loss: 0.9020\n",
            "Epoch 259/500\n",
            "105/105 [==============================] - 0s 179us/sample - loss: 0.7572 - val_loss: 0.9010\n",
            "Epoch 260/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7569 - val_loss: 0.9043\n",
            "Epoch 261/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7578 - val_loss: 0.9024\n",
            "Epoch 262/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7564 - val_loss: 0.9039\n",
            "Epoch 263/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7573 - val_loss: 0.9027\n",
            "Epoch 264/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7558 - val_loss: 0.8998\n",
            "Epoch 265/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7567 - val_loss: 0.9011\n",
            "Epoch 266/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7564 - val_loss: 0.9019\n",
            "Epoch 267/500\n",
            "105/105 [==============================] - 0s 185us/sample - loss: 0.7585 - val_loss: 0.9006\n",
            "Epoch 268/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7557 - val_loss: 0.9013\n",
            "Epoch 269/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7562 - val_loss: 0.9012\n",
            "Epoch 270/500\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.7564 - val_loss: 0.9012\n",
            "Epoch 271/500\n",
            "105/105 [==============================] - 0s 131us/sample - loss: 0.7561 - val_loss: 0.8986\n",
            "Epoch 272/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7572 - val_loss: 0.9019\n",
            "Epoch 273/500\n",
            "105/105 [==============================] - 0s 210us/sample - loss: 0.7576 - val_loss: 0.9022\n",
            "Epoch 274/500\n",
            "105/105 [==============================] - 0s 182us/sample - loss: 0.7564 - val_loss: 0.9030\n",
            "Epoch 275/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7565 - val_loss: 0.9062\n",
            "Epoch 276/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7566 - val_loss: 0.9040\n",
            "Epoch 277/500\n",
            "105/105 [==============================] - 0s 169us/sample - loss: 0.7562 - val_loss: 0.9047\n",
            "Epoch 278/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7564 - val_loss: 0.9045\n",
            "Epoch 279/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7572 - val_loss: 0.9030\n",
            "Epoch 280/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7578 - val_loss: 0.9056\n",
            "Epoch 281/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7564 - val_loss: 0.9059\n",
            "Epoch 282/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7562 - val_loss: 0.9024\n",
            "Epoch 283/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7569 - val_loss: 0.9026\n",
            "Epoch 284/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7566 - val_loss: 0.9053\n",
            "Epoch 285/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7568 - val_loss: 0.9086\n",
            "Epoch 286/500\n",
            "105/105 [==============================] - 0s 195us/sample - loss: 0.7563 - val_loss: 0.9098\n",
            "Epoch 287/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7564 - val_loss: 0.9052\n",
            "Epoch 288/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7560 - val_loss: 0.9060\n",
            "Epoch 289/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7564 - val_loss: 0.9055\n",
            "Epoch 290/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7564 - val_loss: 0.9065\n",
            "Epoch 291/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7563 - val_loss: 0.9088\n",
            "Epoch 292/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7565 - val_loss: 0.9085\n",
            "Epoch 293/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7558 - val_loss: 0.9049\n",
            "Epoch 294/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7563 - val_loss: 0.9063\n",
            "Epoch 295/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7563 - val_loss: 0.9073\n",
            "Epoch 296/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7560 - val_loss: 0.9098\n",
            "Epoch 297/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7557 - val_loss: 0.9081\n",
            "Epoch 298/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7558 - val_loss: 0.9069\n",
            "Epoch 299/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7562 - val_loss: 0.9044\n",
            "Epoch 300/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7559 - val_loss: 0.9005\n",
            "Epoch 301/500\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.7569 - val_loss: 0.9014\n",
            "Epoch 302/500\n",
            "105/105 [==============================] - 0s 213us/sample - loss: 0.7567 - val_loss: 0.9039\n",
            "Epoch 303/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7562 - val_loss: 0.9036\n",
            "Epoch 304/500\n",
            "105/105 [==============================] - 0s 170us/sample - loss: 0.7570 - val_loss: 0.9051\n",
            "Epoch 305/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7577 - val_loss: 0.9047\n",
            "Epoch 306/500\n",
            "105/105 [==============================] - 0s 136us/sample - loss: 0.7585 - val_loss: 0.9046\n",
            "Epoch 307/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7590 - val_loss: 0.9048\n",
            "Epoch 308/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7591 - val_loss: 0.9026\n",
            "Epoch 309/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7584 - val_loss: 0.9019\n",
            "Epoch 310/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7575 - val_loss: 0.9015\n",
            "Epoch 311/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7592 - val_loss: 0.8968\n",
            "Epoch 312/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7610 - val_loss: 0.8980\n",
            "Epoch 313/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7565 - val_loss: 0.9033\n",
            "Epoch 314/500\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.7567 - val_loss: 0.9024\n",
            "Epoch 315/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7571 - val_loss: 0.9084\n",
            "Epoch 316/500\n",
            "105/105 [==============================] - 0s 168us/sample - loss: 0.7573 - val_loss: 0.9122\n",
            "Epoch 317/500\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.7565 - val_loss: 0.9110\n",
            "Epoch 318/500\n",
            "105/105 [==============================] - 0s 180us/sample - loss: 0.7571 - val_loss: 0.9138\n",
            "Epoch 319/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7562 - val_loss: 0.9155\n",
            "Epoch 320/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7563 - val_loss: 0.9153\n",
            "Epoch 321/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7568 - val_loss: 0.9182\n",
            "Epoch 322/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7566 - val_loss: 0.9171\n",
            "Epoch 323/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7570 - val_loss: 0.9248\n",
            "Epoch 324/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7586 - val_loss: 0.9263\n",
            "Epoch 325/500\n",
            "105/105 [==============================] - 0s 134us/sample - loss: 0.7575 - val_loss: 0.9264\n",
            "Epoch 326/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7587 - val_loss: 0.9207\n",
            "Epoch 327/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7565 - val_loss: 0.9193\n",
            "Epoch 328/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7575 - val_loss: 0.9170\n",
            "Epoch 329/500\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.7564 - val_loss: 0.9183\n",
            "Epoch 330/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7571 - val_loss: 0.9210\n",
            "Epoch 331/500\n",
            "105/105 [==============================] - 0s 168us/sample - loss: 0.7573 - val_loss: 0.9192\n",
            "Epoch 332/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7590 - val_loss: 0.9260\n",
            "Epoch 333/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7580 - val_loss: 0.9251\n",
            "Epoch 334/500\n",
            "105/105 [==============================] - 0s 189us/sample - loss: 0.7577 - val_loss: 0.9218\n",
            "Epoch 335/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7568 - val_loss: 0.9220\n",
            "Epoch 336/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7572 - val_loss: 0.9218\n",
            "Epoch 337/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7575 - val_loss: 0.9247\n",
            "Epoch 338/500\n",
            "105/105 [==============================] - 0s 194us/sample - loss: 0.7572 - val_loss: 0.9223\n",
            "Epoch 339/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7568 - val_loss: 0.9182\n",
            "Epoch 340/500\n",
            "105/105 [==============================] - 0s 178us/sample - loss: 0.7571 - val_loss: 0.9159\n",
            "Epoch 341/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7577 - val_loss: 0.9123\n",
            "Epoch 342/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7561 - val_loss: 0.9129\n",
            "Epoch 343/500\n",
            "105/105 [==============================] - 0s 180us/sample - loss: 0.7561 - val_loss: 0.9142\n",
            "Epoch 344/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7574 - val_loss: 0.9131\n",
            "Epoch 345/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7558 - val_loss: 0.9101\n",
            "Epoch 346/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7572 - val_loss: 0.9154\n",
            "Epoch 347/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7565 - val_loss: 0.9146\n",
            "Epoch 348/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7573 - val_loss: 0.9150\n",
            "Epoch 349/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7571 - val_loss: 0.9146\n",
            "Epoch 350/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7568 - val_loss: 0.9147\n",
            "Epoch 351/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7567 - val_loss: 0.9102\n",
            "Epoch 352/500\n",
            "105/105 [==============================] - 0s 190us/sample - loss: 0.7568 - val_loss: 0.9151\n",
            "Epoch 353/500\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.7575 - val_loss: 0.9103\n",
            "Epoch 354/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7569 - val_loss: 0.9108\n",
            "Epoch 355/500\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.7567 - val_loss: 0.9091\n",
            "Epoch 356/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7563 - val_loss: 0.9104\n",
            "Epoch 357/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7574 - val_loss: 0.9055\n",
            "Epoch 358/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7562 - val_loss: 0.9076\n",
            "Epoch 359/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7568 - val_loss: 0.9085\n",
            "Epoch 360/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7572 - val_loss: 0.9081\n",
            "Epoch 361/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7568 - val_loss: 0.9084\n",
            "Epoch 362/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7559 - val_loss: 0.9087\n",
            "Epoch 363/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7570 - val_loss: 0.9089\n",
            "Epoch 364/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7561 - val_loss: 0.9063\n",
            "Epoch 365/500\n",
            "105/105 [==============================] - 0s 273us/sample - loss: 0.7562 - val_loss: 0.9026\n",
            "Epoch 366/500\n",
            "105/105 [==============================] - 0s 199us/sample - loss: 0.7561 - val_loss: 0.9020\n",
            "Epoch 367/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7588 - val_loss: 0.9074\n",
            "Epoch 368/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7570 - val_loss: 0.9076\n",
            "Epoch 369/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7566 - val_loss: 0.9109\n",
            "Epoch 370/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7570 - val_loss: 0.9115\n",
            "Epoch 371/500\n",
            "105/105 [==============================] - 0s 136us/sample - loss: 0.7568 - val_loss: 0.9120\n",
            "Epoch 372/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7565 - val_loss: 0.9120\n",
            "Epoch 373/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7570 - val_loss: 0.9139\n",
            "Epoch 374/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7565 - val_loss: 0.9127\n",
            "Epoch 375/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7574 - val_loss: 0.9120\n",
            "Epoch 376/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7570 - val_loss: 0.9095\n",
            "Epoch 377/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7562 - val_loss: 0.9063\n",
            "Epoch 378/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7560 - val_loss: 0.9045\n",
            "Epoch 379/500\n",
            "105/105 [==============================] - 0s 169us/sample - loss: 0.7572 - val_loss: 0.9045\n",
            "Epoch 380/500\n",
            "105/105 [==============================] - 0s 136us/sample - loss: 0.7566 - val_loss: 0.9076\n",
            "Epoch 381/500\n",
            "105/105 [==============================] - 0s 134us/sample - loss: 0.7567 - val_loss: 0.9081\n",
            "Epoch 382/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7562 - val_loss: 0.9083\n",
            "Epoch 383/500\n",
            "105/105 [==============================] - 0s 184us/sample - loss: 0.7571 - val_loss: 0.9041\n",
            "Epoch 384/500\n",
            "105/105 [==============================] - 0s 180us/sample - loss: 0.7571 - val_loss: 0.9001\n",
            "Epoch 385/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7559 - val_loss: 0.8992\n",
            "Epoch 386/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7571 - val_loss: 0.8985\n",
            "Epoch 387/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7568 - val_loss: 0.8962\n",
            "Epoch 388/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7565 - val_loss: 0.8945\n",
            "Epoch 389/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7578 - val_loss: 0.8945\n",
            "Epoch 390/500\n",
            "105/105 [==============================] - 0s 170us/sample - loss: 0.7568 - val_loss: 0.8949\n",
            "Epoch 391/500\n",
            "105/105 [==============================] - 0s 207us/sample - loss: 0.7570 - val_loss: 0.8961\n",
            "Epoch 392/500\n",
            "105/105 [==============================] - 0s 169us/sample - loss: 0.7562 - val_loss: 0.8983\n",
            "Epoch 393/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7573 - val_loss: 0.9023\n",
            "Epoch 394/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7564 - val_loss: 0.9039\n",
            "Epoch 395/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7561 - val_loss: 0.8996\n",
            "Epoch 396/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7564 - val_loss: 0.8981\n",
            "Epoch 397/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7562 - val_loss: 0.9024\n",
            "Epoch 398/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7560 - val_loss: 0.9006\n",
            "Epoch 399/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7564 - val_loss: 0.9018\n",
            "Epoch 400/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7565 - val_loss: 0.9009\n",
            "Epoch 401/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7572 - val_loss: 0.9027\n",
            "Epoch 402/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7570 - val_loss: 0.9014\n",
            "Epoch 403/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7569 - val_loss: 0.9052\n",
            "Epoch 404/500\n",
            "105/105 [==============================] - 0s 175us/sample - loss: 0.7568 - val_loss: 0.9090\n",
            "Epoch 405/500\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.7560 - val_loss: 0.9105\n",
            "Epoch 406/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7572 - val_loss: 0.9138\n",
            "Epoch 407/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7562 - val_loss: 0.9172\n",
            "Epoch 408/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7576 - val_loss: 0.9127\n",
            "Epoch 409/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7568 - val_loss: 0.9089\n",
            "Epoch 410/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7567 - val_loss: 0.9071\n",
            "Epoch 411/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7574 - val_loss: 0.9044\n",
            "Epoch 412/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7610 - val_loss: 0.9057\n",
            "Epoch 413/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7572 - val_loss: 0.9123\n",
            "Epoch 414/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7566 - val_loss: 0.9090\n",
            "Epoch 415/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7577 - val_loss: 0.9071\n",
            "Epoch 416/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7585 - val_loss: 0.9017\n",
            "Epoch 417/500\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.7575 - val_loss: 0.8975\n",
            "Epoch 418/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7572 - val_loss: 0.8975\n",
            "Epoch 419/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7572 - val_loss: 0.9000\n",
            "Epoch 420/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7566 - val_loss: 0.9006\n",
            "Epoch 421/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7569 - val_loss: 0.9007\n",
            "Epoch 422/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7584 - val_loss: 0.9057\n",
            "Epoch 423/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7563 - val_loss: 0.9100\n",
            "Epoch 424/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7571 - val_loss: 0.9116\n",
            "Epoch 425/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7569 - val_loss: 0.9127\n",
            "Epoch 426/500\n",
            "105/105 [==============================] - 0s 136us/sample - loss: 0.7566 - val_loss: 0.9092\n",
            "Epoch 427/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7566 - val_loss: 0.9060\n",
            "Epoch 428/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7568 - val_loss: 0.9118\n",
            "Epoch 429/500\n",
            "105/105 [==============================] - 0s 132us/sample - loss: 0.7568 - val_loss: 0.9094\n",
            "Epoch 430/500\n",
            "105/105 [==============================] - 0s 132us/sample - loss: 0.7563 - val_loss: 0.9063\n",
            "Epoch 431/500\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.7558 - val_loss: 0.9028\n",
            "Epoch 432/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7571 - val_loss: 0.8989\n",
            "Epoch 433/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7572 - val_loss: 0.9013\n",
            "Epoch 434/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7561 - val_loss: 0.9027\n",
            "Epoch 435/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7567 - val_loss: 0.9053\n",
            "Epoch 436/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7578 - val_loss: 0.9052\n",
            "Epoch 437/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7561 - val_loss: 0.9069\n",
            "Epoch 438/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7565 - val_loss: 0.9092\n",
            "Epoch 439/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7560 - val_loss: 0.9059\n",
            "Epoch 440/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7563 - val_loss: 0.9077\n",
            "Epoch 441/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7561 - val_loss: 0.9062\n",
            "Epoch 442/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7562 - val_loss: 0.9072\n",
            "Epoch 443/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7564 - val_loss: 0.9062\n",
            "Epoch 444/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7558 - val_loss: 0.9111\n",
            "Epoch 445/500\n",
            "105/105 [==============================] - 0s 168us/sample - loss: 0.7562 - val_loss: 0.9115\n",
            "Epoch 446/500\n",
            "105/105 [==============================] - 0s 217us/sample - loss: 0.7587 - val_loss: 0.9128\n",
            "Epoch 447/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7574 - val_loss: 0.9134\n",
            "Epoch 448/500\n",
            "105/105 [==============================] - 0s 170us/sample - loss: 0.7562 - val_loss: 0.9115\n",
            "Epoch 449/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7562 - val_loss: 0.9100\n",
            "Epoch 450/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7560 - val_loss: 0.9093\n",
            "Epoch 451/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7561 - val_loss: 0.9106\n",
            "Epoch 452/500\n",
            "105/105 [==============================] - 0s 134us/sample - loss: 0.7562 - val_loss: 0.9096\n",
            "Epoch 453/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7563 - val_loss: 0.9054\n",
            "Epoch 454/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7568 - val_loss: 0.9071\n",
            "Epoch 455/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7564 - val_loss: 0.9060\n",
            "Epoch 456/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7560 - val_loss: 0.9072\n",
            "Epoch 457/500\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.7566 - val_loss: 0.9072\n",
            "Epoch 458/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7567 - val_loss: 0.9030\n",
            "Epoch 459/500\n",
            "105/105 [==============================] - 0s 191us/sample - loss: 0.7568 - val_loss: 0.9014\n",
            "Epoch 460/500\n",
            "105/105 [==============================] - 0s 200us/sample - loss: 0.7561 - val_loss: 0.9001\n",
            "Epoch 461/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7563 - val_loss: 0.8985\n",
            "Epoch 462/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7564 - val_loss: 0.8999\n",
            "Epoch 463/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7564 - val_loss: 0.8959\n",
            "Epoch 464/500\n",
            "105/105 [==============================] - 0s 131us/sample - loss: 0.7563 - val_loss: 0.8950\n",
            "Epoch 465/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7564 - val_loss: 0.8954\n",
            "Epoch 466/500\n",
            "105/105 [==============================] - 0s 125us/sample - loss: 0.7567 - val_loss: 0.8982\n",
            "Epoch 467/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7575 - val_loss: 0.8976\n",
            "Epoch 468/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7566 - val_loss: 0.8977\n",
            "Epoch 469/500\n",
            "105/105 [==============================] - 0s 173us/sample - loss: 0.7572 - val_loss: 0.8985\n",
            "Epoch 470/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7570 - val_loss: 0.8967\n",
            "Epoch 471/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7560 - val_loss: 0.8980\n",
            "Epoch 472/500\n",
            "105/105 [==============================] - 0s 182us/sample - loss: 0.7565 - val_loss: 0.8961\n",
            "Epoch 473/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7565 - val_loss: 0.8947\n",
            "Epoch 474/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7564 - val_loss: 0.8994\n",
            "Epoch 475/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7561 - val_loss: 0.8974\n",
            "Epoch 476/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7562 - val_loss: 0.8979\n",
            "Epoch 477/500\n",
            "105/105 [==============================] - 0s 169us/sample - loss: 0.7559 - val_loss: 0.8986\n",
            "Epoch 478/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7560 - val_loss: 0.8991\n",
            "Epoch 479/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7563 - val_loss: 0.8952\n",
            "Epoch 480/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7586 - val_loss: 0.8932\n",
            "Epoch 481/500\n",
            "105/105 [==============================] - 0s 179us/sample - loss: 0.7567 - val_loss: 0.8936\n",
            "Epoch 482/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7567 - val_loss: 0.9005\n",
            "Epoch 483/500\n",
            "105/105 [==============================] - 0s 191us/sample - loss: 0.7558 - val_loss: 0.8996\n",
            "Epoch 484/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7570 - val_loss: 0.8988\n",
            "Epoch 485/500\n",
            "105/105 [==============================] - 0s 169us/sample - loss: 0.7565 - val_loss: 0.9048\n",
            "Epoch 486/500\n",
            "105/105 [==============================] - 0s 186us/sample - loss: 0.7562 - val_loss: 0.9059\n",
            "Epoch 487/500\n",
            "105/105 [==============================] - 0s 178us/sample - loss: 0.7563 - val_loss: 0.9053\n",
            "Epoch 488/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7563 - val_loss: 0.9063\n",
            "Epoch 489/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7561 - val_loss: 0.9048\n",
            "Epoch 490/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7576 - val_loss: 0.9056\n",
            "Epoch 491/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7569 - val_loss: 0.9012\n",
            "Epoch 492/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7557 - val_loss: 0.8982\n",
            "Epoch 493/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7567 - val_loss: 0.8971\n",
            "Epoch 494/500\n",
            "105/105 [==============================] - 0s 170us/sample - loss: 0.7566 - val_loss: 0.8954\n",
            "Epoch 495/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7562 - val_loss: 0.8995\n",
            "Epoch 496/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7561 - val_loss: 0.8990\n",
            "Epoch 497/500\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.7565 - val_loss: 0.9038\n",
            "Epoch 498/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7557 - val_loss: 0.9037\n",
            "Epoch 499/500\n",
            "105/105 [==============================] - 0s 173us/sample - loss: 0.7570 - val_loss: 0.9067\n",
            "Epoch 500/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7563 - val_loss: 0.9043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyvdgqR4mjju",
        "colab_type": "code",
        "outputId": "0838eeb3-658c-4d02-844d-4a88c87740e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# 시각화\n",
        "epochs = np.arange(1, 500 + 1)\n",
        "plt.plot(epochs, history.history['loss'], label = 'Training loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAenklEQVR4nO3de3Bc5Z3m8e+vz+m77hcbYxlswFxk\ngo2jCWGhiksyGSCbZDchO0AImRSz1FaxyWxl2B0nyw4ZUpUipGZIyJALO0uobFiY7JDZYagkzo2E\nybIBbC42YIwNGCzfJMuy7urru390tywL2ZLtlptz+vlUqdR9+nSf91V3P/r1e97Tx5xziIhI8EVq\n3QAREakOBbqISEgo0EVEQkKBLiISEgp0EZGQ8Gu14Y6ODrd8+fJabV5EJJA2bty43znXOdttNQv0\n5cuXs2HDhlptXkQkkMzsrSPdpiEXEZGQUKCLiISEAl1EJCRqNoYuIu8+uVyO3t5eJicna92UupdI\nJOjq6iIajc77Pgp0EZnS29tLY2Mjy5cvx8xq3Zy65ZxjYGCA3t5eVqxYMe/7achFRKZMTk7S3t6u\nMK8xM6O9vf2YPykp0EXkMArzd4fjeR4CF+hb947w1z/fysBoptZNERF5VwlcoL/eP8q3fr2d/aPZ\nWjdFRKpsYGCANWvWsGbNGk455RSWLl06dT2bnd97/rOf/Sxbt2496jr33XcfDz30UDWazKWXXsoL\nL7xQlcc6UYHbKepHSh9DcoVijVsiItXW3t4+FY5f/vKXaWho4LbbbjtsHecczjkikdnr0e9///tz\nbufWW2898ca+CwWuQo/6pSYr0EXqx/bt2+nu7uZTn/oUq1atYs+ePdxyyy309PSwatUq7rzzzql1\nKxVzPp+npaWFdevWsXr1ai6++GL6+voAuP322/nGN74xtf66det43/vexznnnMNTTz0FwNjYGJ/4\nxCfo7u7m2muvpaenZ85K/Ic//CHvec97OP/88/nSl74EQD6f59Of/vTU8nvvvReAe+65h+7ubi64\n4AJuvPHGqvydAlehRyOVQNep80QW0l/988u8snu4qo/ZfWoTd3xk1XHd99VXX+UHP/gBPT09ANx1\n1120tbWRz+e54ooruPbaa+nu7j7sPkNDQ1x22WXcddddfOELX+CBBx5g3bp173hs5xzPPPMMjz32\nGHfeeSc/+9nP+Na3vsUpp5zCo48+yosvvsjatWuP2r7e3l5uv/12NmzYQHNzMx/84Ad5/PHH6ezs\nZP/+/WzevBmAgwcPAnD33Xfz1ltvEYvFppadqMBV6L5XGnLJq0IXqStnnnnmVJgDPPzww6xdu5a1\na9eyZcsWXnnllXfcJ5lMcvXVVwPw3ve+lx07dsz62B//+Mffsc7vfvc7rrvuOgBWr17NqlVH/0f0\n9NNPc+WVV9LR0UE0GuWGG27gySef5KyzzmLr1q18/vOfZ/369TQ3NwOwatUqbrzxRh566KFjOnjo\naIJXoXul/0FZBbrIgjreSnqhpNPpqcvbtm3jm9/8Js888wwtLS3ceOONs87ZjsViU5c9zyOfz8/6\n2PF4fM51jld7ezubNm3ipz/9Kffddx+PPvoo999/P+vXr+e3v/0tjz32GF/96lfZtGkTnued0LYC\nV6FHpyp0DbmI1Kvh4WEaGxtpampiz549rF+/vurbuOSSS/jRj34EwObNm2f9BDDdRRddxBNPPMHA\nwAD5fJ5HHnmEyy67jP7+fpxzfPKTn+TOO+/kueeeo1Ao0Nvby5VXXsndd9/N/v37GR8fP+E2B7ZC\n105Rkfq1du1auru7Offcczn99NO55JJLqr6Nz33uc9x00010d3dP/VSGS2bT1dXFV77yFS6//HKc\nc3zkIx/hwx/+MM899xw333wzzjnMjK997Wvk83luuOEGRkZGKBaL3HbbbTQ2Np5wm8252lS6PT09\n7nhOcLG9b4QP/s2T3Hv9hXx09akL0DKR+rVlyxbOO++8WjfjXSGfz5PP50kkEmzbto0PfehDbNu2\nDd8/eXXwbM+HmW10zvXMtn5wK/S8KnQRWTijo6N84AMfIJ/P45zje9/73kkN8+Px7m7dLPxyoOeL\nCnQRWTgtLS1s3Lix1s04JoHdKap56CILo1bDsHK443ke5gx0M3vAzPrM7KWjrHO5mb1gZi+b2W+P\nuRXH4NCBRarQRaotkUgwMDCgUK+xyvehJxKJY7rffIZcHgT+FvjBbDeaWQvwbeAq59zbZrbomFpw\njCqH/mvaokj1dXV10dvbS39/f62bUvcqZyw6FnMGunPuSTNbfpRVbgB+7Jx7u7x+3zG14BhVvpxL\nBxaJVF80Gj2mM+TIu0s1xtDPBlrN7DdmttHMbjrSimZ2i5ltMLMNx1sBVGa5qEIXETlcNQLdB94L\nfBj4I+C/mdnZs63onLvfOdfjnOvp7Ow8ro15ESNiGkMXEZmpGtMWe4EB59wYMGZmTwKrgdeq8Niz\n8r0IOU1bFBE5TDUq9H8CLjUz38xSwEXAlio87hHFvAi5vIZcRESmm7NCN7OHgcuBDjPrBe4AogDO\nue8657aY2c+ATUAR+Dvn3BGnOFaD75kOLBIRmWE+s1yun8c6Xwe+XpUWzUPUi2gMXURkhsAdKQoQ\njZiOFBURmSGYge6rQhcRmSmQge5HTPPQRURmCGSgawxdROSdFOgiIiER0EA38kUNuYiITBfIQPe9\nCFmdsUhE5DCBDPSYF1GFLiIyQyAD3fdMY+giIjMEM9AjER1YJCIyQyADPearQhcRmSmQge5HIuQV\n6CIihwlkoJfmoWvIRURkuoAGuoZcRERmCmig60hREZGZAhnovqcv5xIRmSmQgR7TOUVFRN4hkIFe\nOrBIFbqIyHSBDPSoF6FQdBR1+L+IyJTABjqgYRcRkWkCGugGoB2jIiLTBDLQ/Ui5QtfURRGRKYEM\n9EqFrh2jIiKHBDTQVaGLiMwUyED3y4GuMXQRkUMCGeiVIZesKnQRkSkBDfRyha5piyIiUwId6Lm8\nhlxERCoCGeh+ZZaLKnQRkSmBDPSYdoqKiLxDIAPdj1TmoatCFxGpCGSgR33NQxcRmSmYgT516L+G\nXEREKoIZ6H7ly7lUoYuIVMwZ6Gb2gJn1mdlLc6z3B2aWN7Nrq9e82VW+nEsHFomIHDKfCv1B4Kqj\nrWBmHvA14OdVaNOc9PW5IiLvNGegO+eeBA7MsdrngEeBvmo0ai6VA4tUoYuIHHLCY+hmthT4t8B3\nTrw58xMvz3LJ5hXoIiIV1dgp+g3gL5xzc6armd1iZhvMbEN/f/9xbzAe9QAFuojIdH4VHqMHeMTM\nADqAa8ws75z7PzNXdM7dD9wP0NPTc9wD4JUKPZMvHO9DiIiEzgkHunNuReWymT0IPD5bmFeTHzEi\nBhlV6CIiU+YMdDN7GLgc6DCzXuAOIArgnPvugrbuyG0i5kcU6CIi08wZ6M656+f7YM65Pzmh1hyD\nuO+RyWnIRUSkIpBHikJpHF3TFkVEDgluoEcjZHIKdBGRiuAGuu9pDF1EZJrABnrMi2jaoojINIEN\n9HhUs1xERKYLbqD7GkMXEZkuwIHukdEsFxGRKQEO9IjmoYuITBPYQI/5EX05l4jINIENdE1bFBE5\nXHADPappiyIi0wU30PXlXCIihwlwoGvIRURkusAGemWnqHM6UbSICAQ40A+dtUhVuogIKNBFREIj\nuIGuE0WLiBwmuIGuE0WLiBwmBIGuCl1EBMIQ6PrGRRERINCBXhpD15CLiEhJgAO91HTtFBURKQlu\noEc1hi4iMl1gAz3mVYZcFOgiIhDgQD9UoWsMXUQEghzomuUiInKYAAd6+UhRnVdURAQIdKBXKnQN\nuYiIQIADPaYjRUVEDhPYQNeh/yIihwtsoPteBC9imuUiIlIW2ECHUpWuI0VFREoCH+gachERKQl4\noHtMapaLiAgQ8EBPxjzGswp0ERGYR6Cb2QNm1mdmLx3h9k+Z2SYz22xmT5nZ6uo3c3bJqMeEAl1E\nBJhfhf4gcNVRbn8TuMw59x7gK8D9VWjXvKRUoYuITJkz0J1zTwIHjnL7U865wfLV3wNdVWrbnJIx\nj3GNoYuIANUfQ78Z+OmRbjSzW8xsg5lt6O/vP+GNpWIeE9n8CT+OiEgYVC3QzewKSoH+F0daxzl3\nv3OuxznX09nZecLbTMV8DbmIiJT51XgQM7sA+DvgaufcQDUecz6SMe0UFRGpOOEK3cxOA34MfNo5\n99qJN2n+UlHtFBURqZizQjezh4HLgQ4z6wXuAKIAzrnvAn8JtAPfNjOAvHOuZ6EaPF0q5jGRK1As\nOiIROxmbFBF515oz0J1z189x+58Cf1q1Fh2DZKzU/Ml8gVSsKqNHIiKBFegjRVOx0lmLNOwiIhLw\nQE+WA107RkVEAh7o6fIwiyp0EZGAB/qhIRcdXCQiEuhA15CLiMghgQ507RQVETkkHIGuL+gSEQl2\noFfmoesLukREAh7oqaiGXEREKgId6EmNoYuITAl0oMf9CBHTLBcREQh4oJsZqZjPmMbQRUSCHeig\n70QXEakIfKDrRNEiIiWBD/SkTnIhIgKEINBLJ7nQGLqISAgCXSeKFhGBEAS6doqKiJQEPtDT2ikq\nIgKEINBTcV/fhy4iQggCPR3zGMuoQhcRCXygp2I+E7kChaKrdVNERGoq8IGejpfPWqTvRBeROhf4\nQE9VThSd0Ti6iNS3wAd6pUIf00wXEalzgQ/0ZLRUoY+pQheROhf4QNcYuohISeADvTKGrgpdROpd\n4AO9UqHraFERqXfBD3RV6CIiQAgCPVU+UbQCXUTqXeADvTERBWBoQoEuIvUt8IEe8yM0xH0Gx7O1\nboqISE0FPtABWlJRDirQRaTOzRnoZvaAmfWZ2UtHuN3M7F4z225mm8xsbfWbeXStqRiD47mTvVkR\nkXeV+VToDwJXHeX2q4GV5Z9bgO+ceLOOjSp0EZF5BLpz7kngwFFW+RjwA1fye6DFzJZUq4HzoQpd\nRKQ6Y+hLgZ3TrveWl72Dmd1iZhvMbEN/f38VNl3Smopqp6iI1L2TulPUOXe/c67HOdfT2dlZtcdt\nScUYmcyTLxSr9pgiIkFTjUDfBSybdr2rvOykaU2V5qIfnNCwi4jUr2oE+mPATeXZLu8Hhpxze6rw\nuPPWmo4BaMeoiNQ1f64VzOxh4HKgw8x6gTuAKIBz7rvAT4BrgO3AOPDZhWrskbSkSoGuHaMiUs/m\nDHTn3PVz3O6AW6vWouNQGXIZHFOFLiL1KxRHiramKkMuqtBFpH6FItBbKhW6xtBFpI6FItAb4j5+\nxDSGLiJ1LRSBbma0pGKa5SIidS0UgQ7Qno6xf1SBLiL1KzSBvqgpTv/IZK2bISJSM6EJ9FOaEuwb\nztS6GSIiNROaQF/clKB/NEOh6GrdFBGRmghPoDcnKBQdA6Oq0kWkPoUn0BvjAOwd1ji6iNSn0AT6\n0tYkADsPTNS4JSIitRGaQD+jowEz2N43WuumiIjURGgCPRnzWNaaYlvfSK2bIiJSE6EJdICzFjWo\nQheRuhWqQF+5qIE39o/pVHQiUpdCFehnLWogmy+yc1A7RkWk/oQu0AG27dM4uojUn1AG+vZ+jaOL\nSP0JVaA3JqIsaU6wfZ8CXUTqT6gCHUpV+jbNdBGROhTKQH+9f5SivqRLROpMKAN9PFtg95BmuohI\nfQldoK9c1AjoKwBEpP6EMNDLM10U6CJSZ0IX6K3pGO3pGNs000VE6kzoAh3K3+miuegiUmdCGegr\nFzewbd8Izmmmi4jUj1AG+lmdDQxP5ukf0enoRKR+hDLQVy4uzXTRAUYiUk9CGejdS5oA2NQ7VOOW\niIicPKEM9NZ0jBUdaZ57e7DWTREROWlCGegAF57WwvNvD2rHqIjUjRAHeiv7R7PsPKCvABCR+hDa\nQF97WguAhl1EpG7MK9DN7Coz22pm281s3Sy3n2ZmT5jZ82a2ycyuqX5Tj805ixtJxTyeV6CLSJ2Y\nM9DNzAPuA64GuoHrzax7xmq3Az9yzl0IXAd8u9oNPVa+F2F1VwvPvX2w1k0RETkp5lOhvw/Y7px7\nwzmXBR4BPjZjHQc0lS83A7ur18Tjt/b0Fl7ZM8x4Nl/rpoiILLj5BPpSYOe0673lZdN9GbjRzHqB\nnwCfm+2BzOwWM9tgZhv6+/uPo7nH5pIzOygUHY9v2rPg2xIRqbVq7RS9HnjQOdcFXAP8TzN7x2M7\n5+53zvU453o6OzurtOkju/jMds5b0sS3fr2NiWxhwbcnIlJL8wn0XcCyade7ysumuxn4EYBz7v8B\nCaCjGg08EWbGHR/pZueBCb75q221bo6IyIKaT6A/C6w0sxVmFqO00/OxGeu8DXwAwMzOoxToCz+m\nMg/vP6OdP+5Zxn//lzd4ebe+CkBEwmvOQHfO5YH/CKwHtlCazfKymd1pZh8tr/bnwL83sxeBh4E/\nce+iQzS/eM25tKaifPb7z/LU9v21bo6IyIKwWuVuT0+P27Bhw0nb3s9e2sOt/+t5CkXHf/6jc7j1\nirNO2rZFRKrFzDY653pmuy20R4rOdNX5S3ju9j/k36w5la+v38qd//wKkzntKBWR8PBr3YCTqTkV\n5a//3Rqak1Ee+L9v8uS2fr7ysfO5aEUb2UKRRNSrdRNFZBrnHGZW62YERt0Mucz029f6+eKjm9g9\nNEnMj5ArFDmtLcVpbSnOXtzIyGSOooPT2lJ0NMR5z9JmMvkCm3cN0TeS4YPnLeasRQ00J6Ns7xsl\nky/Q1ZJie/8oe4Ym2NQ7xLM7DnBGRwMx37j0rE5Ob0/x/M6DRCPGqlObWdGZpiHus/PAOKmYR0PC\nJ+ZFyBcdW/eO8JPNe/j8B1ayvW+UfNHR1ZpkIltgrHygVDLqsagxQSIa4cBYloJzvLJ7mOXtaVJx\nj450nF0HJ+hoiGMGE9kCMT9C3C99MDswlmX/aJZ/2NjLGZ1pXt49zJplzaxc3MgZHWle2T3MsrYU\nAB0NccazefxIhP7RSZY0JxmayJGKeSRjHlv2jLC0JUlzMsqre4fpak3Rlo7hnOPZHYNEPWNFR5rf\nbO0n5kfYNzxJKuaxY2CcX76yjz/+g2Wcc0ojY5kCnY0xlrWlWNSYIJMvsGtwgvZ0nGyhSP9IhuZU\nlJZklJgf4ZXdw/ie0ZKK0ZKMkop5mBnOOYYmciSiHplckUyhwG+29uOZceFpLZzenmYsm2fL7mHi\nUY/VXc28uX+MfcMZDo5n6WiMs6gxDoBhmMHAWBaAxU1x4r7Hr7bs47JzOulsiPP2gXEOjGU5e3Ej\nDpjMFXht3wgXdLUwlsnjR4zmZBQvciicZgaVc46BsSwNcZ+Xdw/z+zcG2HVwgrMXNfCTzXvJForc\ne92FjOfypKI+i5pKf5PdByd4+o0DOOdY0dlAV2uSnQfGeWnXEOctaaKrNcXIZI62dIy+kczU6zsR\n9RjPFhiZLD2PYExkCyxrS7J/NEtnQ5yBsQyntiSZzBVIxXx++Pu36FneSmsqRkPcZzxXYGg8xynN\nCTwzxnOl1+ZYJk9rKsZ4tkAmX2BkMs+S5iRj2TyntaWIehGccwyO5+gfydDREKMlFWPP0ASDYzma\nk1Hu+eVr7BgY4/YPd/Pq3mFWdKRpScYYz5bORnbukiaSUY9ENELRQVPCZ8fAGJ2NCd7cP8ZpbSla\nklGm/5n/8fldNCejXHHOIiLl56JYLGVgvug4OJ6lvSGOFym9hkYzeXbsHycRjRDzI5zWliKTLwLg\nRYxcocg/bOzl4jPaWdKSZDybp7Oh9Lp5de8IS5oTeBGjWATfM3zPGMsUaEvHjju7jjbkUreBDqU3\n3d8/u5O3BsZpiHu83j/Gy7uH2DEwPvUimOvP05aOcaD8Rp8u6hm5QunO6ZjH2Czz4ON+hHTcP+z+\nMS9CtlA8pn7E/AjZ/DvvEzEoztL++fbtRPkRw4vY1BvgSO05krgfmbrvfMW80j+siVyB/LSNzdx2\nY8JnInv4OsfjWJ+v6c+VHym9wf1IBN8zikXH8OThRzU3JXyGJ/M0JnxGJmt7xPPxPB+z8SKGXw7T\najxexWyvLy9iFIoOM4h6h/72lcCurB+x0uHuzpWeU2DW57XyeEdztPtX/IfLzmTd1efOs2eHO1qg\n19WQy0yJqMdn/tXyw5Y558gWikQjEcxg7/AkuwYn2DM0SVMySns6xuv9o5gZbw+M0Ts4wZmdDTQk\nfIYmcixtSbKiI83KxQ2MZQocGMuwvD3NMzsOMDiW45xTGhnP5nlx50G27hvBOWhI+EQjpRfBWDZP\nezpG1IswUj4vams6RvepTewbmmQ0k2dFRxozyBUc+4Yn2T+aYXFTgslcgQu6mtk1OEG+6HhrYJx0\n3MMwxrMFljQn2Ds8SdSLEPOMzqYEuXyRpa1JDoxlaU5GifsRcgXH6/2jpQqsXGH5ESMR9XAO2hti\n7D44QSLqUXSO8fI/q7jvkS8UMSu9MSbzBTK5Iss70kzmCvSNZFh7WguLmxJ0NMQZy+ZxDlZ0pNkz\nNMmb+0cZyxQwg33DGXoHx4n7Hqe3pxidzGMGi5oSjGfyDE3kmMwV6WpNko77HBzPcnAix8HxHBPZ\nPIlY6RPK4HiWhnJ4t6ZibOsbZVFjnD1DE6RiPl2tSYYn8xSLpWoM4LKzO6f67VzpjQ7gRWA0U2Df\n0CRxP0IkYuwdmqQ5GaUtHaMpGWX/aAaj9MZf3JRge98oLakoAEMTOcazBRJ+BMzIF4oUio5cwVEo\nFskXHcvaUkzmCpy9uJGLVrTRlo7x1sA4bQ0x/uW1/YxM5ohHIwyN5xgcz9GY8GlORjl7cSP5YpE3\n+scYzxboak2y6tRmdgyMsWdogpZUjMGxLKmYz+ZdBzFK/0za0zEWNyVK/wALjnTcY+eBCYrOkS86\nOhvivDkwhh8xJnMFzuhsoFLwjmbyhxUlg+Olqj7meyRjEd7sH6OzMV6qnpM+B8Zy5ao8S77gKDrH\n4qYEi5oSDIxm6BvJkI55NCej+F6EZa0pUnGP7X2jnH9qM4PjWUYmS8/7SCZPzDMKRZgo7wvbNTjB\nWYsa2Dc8SUsqytBEjky+OPVpYHgix4qONK3pGFv3juBFDDMjYpArFPEjEdrSMXYPTRAxIxoxGhI+\n7ek4+WKRTL7IroMTNCejOFeq7AvOcWpL6ZPzroMTLGqM0z+aKb1P0jFGJvNEvQiNCZ9c+dNUcyrG\nRSvaFiTT6rpCFxEJGs1yERGpAwp0EZGQUKCLiISEAl1EJCQU6CIiIaFAFxEJCQW6iEhIKNBFREKi\nZgcWmVk/8NZx3r0DqLcvNlef64P6XB9OpM+nO+dmPYdnzQL9RJjZhiMdKRVW6nN9UJ/rw0L1WUMu\nIiIhoUAXEQmJoAb6/bVuQA2oz/VBfa4PC9LnQI6hi4jIOwW1QhcRkRkU6CIiIRGoQDezq8xsq5lt\nN7N1tW5PtZjZA2bWZ2YvTVvWZma/MLNt5d+t5eVmZveW/wabzGxt7Vp+/MxsmZk9YWavmNnLZvZn\n5eWh7beZJczsGTN7sdznvyovX2FmT5f79vdmFisvj5evby/fvryW7T8RZuaZ2fNm9nj5eqj7bGY7\nzGyzmb1gZhvKyxb8tR2YQDczD7gPuBroBq43s+7atqpqHgSumrFsHfAr59xK4Ffl61Dq/8ryzy3A\nd05SG6stD/y5c64beD9wa/n5DHO/M8CVzrnVwBrgKjN7P/A14B7n3FnAIHBzef2bgcHy8nvK6wXV\nnwFbpl2vhz5f4ZxbM22++cK/tp1zgfgBLgbWT7v+ReCLtW5XFfu3HHhp2vWtwJLy5SXA1vLl7wHX\nz7ZekH+AfwL+sF76DaSA54CLKB0x6JeXT73OgfXAxeXLfnk9q3Xbj6OvXeUAuxJ4HLA66PMOoGPG\nsgV/bQemQgeWAjunXe8tLwurxc65PeXLe4HF5cuh+zuUP1ZfCDxNyPtdHnp4AegDfgG8Dhx0zuXL\nq0zv11Sfy7cPAe0nt8VV8Q3gvwDF8vV2wt9nB/zczDaa2S3lZQv+2vaP505ycjnnnJmFcn6pmTUA\njwL/yTk3bGZTt4Wx3865ArDGzFqAfwTOrXGTFpSZ/Wugzzm30cwur3V7TqJLnXO7zGwR8Asze3X6\njQv12g5Shb4LWDbteld5WVjtM7MlAOXffeXlofk7mFmUUpg/5Jz7cXlx6PsN4Jw7CDxBabihxcwq\nxdX0fk31uXx7MzBwkpt6oi4BPmpmO4BHKA27fJNw9xnn3K7y7z5K/7jfx0l4bQcp0J8FVpb3jseA\n64DHatymhfQY8Jny5c9QGmOuLL+pvGf8/cDQtI9xgWGlUvx/AFucc38z7abQ9tvMOsuVOWaWpLTP\nYAulYL+2vNrMPlf+FtcCv3blQdagcM590TnX5ZxbTuk9+2vn3KcIcZ/NLG1mjZXLwIeAlzgZr+1a\n7zw4xh0N1wCvURp3/K+1bk8V+/UwsAfIURo/u5nSuOGvgG3AL4G28rpGabbP68BmoKfW7T/OPl9K\naZxxE/BC+eeaMPcbuAB4vtznl4C/LC8/A3gG2A78byBeXp4oX99evv2MWvfhBPt/OfB42Ptc7tuL\n5Z+XK1l1Ml7bOvRfRCQkgjTkIiIiR6FAFxEJCQW6iEhIKNBFREJCgS4iEhIKdBGRkFCgi4iExP8H\n20g+5ic0So0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqSoJqHosWlU",
        "colab_type": "text"
      },
      "source": [
        "## 함수형 API\n",
        "복잡한 모델을 만들때 이용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SakcaT1JnFlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model, Input\n",
        "\n",
        "input = tf.keras.Input(shape = (1,))\n",
        "output = tf.keras.layers.Dense(1)(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-mTSpURs9uO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dense = tf.keras.layers.Dense(1)\n",
        "output = dense(input)\n",
        "\n",
        "# 다르게도 쓸 수 있음\n",
        "# dense = tf.keras.layers.Dense(1)\n",
        "# ouput = dense.__call__(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPsOb77NtJAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Model(input, output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F67dvDdVtRVl",
        "colab_type": "code",
        "outputId": "06dae996-63ab-473a-eac5-97d60c7d4cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEHWIqA1tTg9",
        "colab_type": "code",
        "outputId": "5279ce5a-2b6f-422e-fe50-34733c52ea20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 나머지는 기존의 sequential model과 동일\n",
        "model.compile(optimizer = 'sgd', loss = 'mse')\n",
        "history = model.fit(x_train, y_train, epochs = 500, validation_split = 0.3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 45 samples\n",
            "Epoch 1/500\n",
            "105/105 [==============================] - 0s 2ms/sample - loss: 1.6597 - val_loss: 1.4337\n",
            "Epoch 2/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 1.5217 - val_loss: 1.3011\n",
            "Epoch 3/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 1.3848 - val_loss: 1.2167\n",
            "Epoch 4/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 1.2892 - val_loss: 1.1527\n",
            "Epoch 5/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 1.2128 - val_loss: 1.0988\n",
            "Epoch 6/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 1.1473 - val_loss: 1.0556\n",
            "Epoch 7/500\n",
            "105/105 [==============================] - 0s 218us/sample - loss: 1.0942 - val_loss: 1.0196\n",
            "Epoch 8/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 1.0483 - val_loss: 0.9822\n",
            "Epoch 9/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 1.0014 - val_loss: 0.9583\n",
            "Epoch 10/500\n",
            "105/105 [==============================] - 0s 136us/sample - loss: 0.9655 - val_loss: 0.9372\n",
            "Epoch 11/500\n",
            "105/105 [==============================] - 0s 132us/sample - loss: 0.9334 - val_loss: 0.9210\n",
            "Epoch 12/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.9085 - val_loss: 0.9070\n",
            "Epoch 13/500\n",
            "105/105 [==============================] - 0s 215us/sample - loss: 0.8852 - val_loss: 0.8960\n",
            "Epoch 14/500\n",
            "105/105 [==============================] - 0s 134us/sample - loss: 0.8663 - val_loss: 0.8879\n",
            "Epoch 15/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.8507 - val_loss: 0.8815\n",
            "Epoch 16/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.8388 - val_loss: 0.8761\n",
            "Epoch 17/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.8255 - val_loss: 0.8725\n",
            "Epoch 18/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.8148 - val_loss: 0.8700\n",
            "Epoch 19/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.8077 - val_loss: 0.8706\n",
            "Epoch 20/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7978 - val_loss: 0.8706\n",
            "Epoch 21/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7943 - val_loss: 0.8722\n",
            "Epoch 22/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7867 - val_loss: 0.8737\n",
            "Epoch 23/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7806 - val_loss: 0.8729\n",
            "Epoch 24/500\n",
            "105/105 [==============================] - 0s 180us/sample - loss: 0.7761 - val_loss: 0.8725\n",
            "Epoch 25/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7745 - val_loss: 0.8715\n",
            "Epoch 26/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7699 - val_loss: 0.8719\n",
            "Epoch 27/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7707 - val_loss: 0.8736\n",
            "Epoch 28/500\n",
            "105/105 [==============================] - 0s 180us/sample - loss: 0.7671 - val_loss: 0.8725\n",
            "Epoch 29/500\n",
            "105/105 [==============================] - 0s 298us/sample - loss: 0.7663 - val_loss: 0.8759\n",
            "Epoch 30/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7637 - val_loss: 0.8762\n",
            "Epoch 31/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7643 - val_loss: 0.8757\n",
            "Epoch 32/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7644 - val_loss: 0.8766\n",
            "Epoch 33/500\n",
            "105/105 [==============================] - 0s 190us/sample - loss: 0.7628 - val_loss: 0.8771\n",
            "Epoch 34/500\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.7622 - val_loss: 0.8781\n",
            "Epoch 35/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7621 - val_loss: 0.8799\n",
            "Epoch 36/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7599 - val_loss: 0.8821\n",
            "Epoch 37/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7594 - val_loss: 0.8823\n",
            "Epoch 38/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7599 - val_loss: 0.8871\n",
            "Epoch 39/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7580 - val_loss: 0.8881\n",
            "Epoch 40/500\n",
            "105/105 [==============================] - 0s 210us/sample - loss: 0.7575 - val_loss: 0.8913\n",
            "Epoch 41/500\n",
            "105/105 [==============================] - 0s 188us/sample - loss: 0.7578 - val_loss: 0.8918\n",
            "Epoch 42/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7576 - val_loss: 0.8907\n",
            "Epoch 43/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7579 - val_loss: 0.8907\n",
            "Epoch 44/500\n",
            "105/105 [==============================] - 0s 188us/sample - loss: 0.7579 - val_loss: 0.8962\n",
            "Epoch 45/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7569 - val_loss: 0.8972\n",
            "Epoch 46/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7560 - val_loss: 0.8984\n",
            "Epoch 47/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7567 - val_loss: 0.8961\n",
            "Epoch 48/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7565 - val_loss: 0.8980\n",
            "Epoch 49/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7570 - val_loss: 0.8965\n",
            "Epoch 50/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7569 - val_loss: 0.8966\n",
            "Epoch 51/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7575 - val_loss: 0.8984\n",
            "Epoch 52/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7563 - val_loss: 0.9029\n",
            "Epoch 53/500\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 0.7558 - val_loss: 0.9034\n",
            "Epoch 54/500\n",
            "105/105 [==============================] - 0s 186us/sample - loss: 0.7565 - val_loss: 0.8995\n",
            "Epoch 55/500\n",
            "105/105 [==============================] - 0s 194us/sample - loss: 0.7563 - val_loss: 0.9015\n",
            "Epoch 56/500\n",
            "105/105 [==============================] - 0s 212us/sample - loss: 0.7580 - val_loss: 0.9020\n",
            "Epoch 57/500\n",
            "105/105 [==============================] - 0s 200us/sample - loss: 0.7562 - val_loss: 0.9033\n",
            "Epoch 58/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7567 - val_loss: 0.9024\n",
            "Epoch 59/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7565 - val_loss: 0.9060\n",
            "Epoch 60/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7561 - val_loss: 0.9111\n",
            "Epoch 61/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7560 - val_loss: 0.9124\n",
            "Epoch 62/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7569 - val_loss: 0.9071\n",
            "Epoch 63/500\n",
            "105/105 [==============================] - 0s 132us/sample - loss: 0.7561 - val_loss: 0.9059\n",
            "Epoch 64/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7573 - val_loss: 0.9044\n",
            "Epoch 65/500\n",
            "105/105 [==============================] - 0s 124us/sample - loss: 0.7567 - val_loss: 0.9010\n",
            "Epoch 66/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7565 - val_loss: 0.9060\n",
            "Epoch 67/500\n",
            "105/105 [==============================] - 0s 123us/sample - loss: 0.7570 - val_loss: 0.9063\n",
            "Epoch 68/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7561 - val_loss: 0.9078\n",
            "Epoch 69/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7563 - val_loss: 0.9061\n",
            "Epoch 70/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7569 - val_loss: 0.9036\n",
            "Epoch 71/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7579 - val_loss: 0.8984\n",
            "Epoch 72/500\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.7572 - val_loss: 0.8968\n",
            "Epoch 73/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7565 - val_loss: 0.8965\n",
            "Epoch 74/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7577 - val_loss: 0.8970\n",
            "Epoch 75/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7577 - val_loss: 0.8957\n",
            "Epoch 76/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7565 - val_loss: 0.8951\n",
            "Epoch 77/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7573 - val_loss: 0.8947\n",
            "Epoch 78/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7567 - val_loss: 0.8931\n",
            "Epoch 79/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7581 - val_loss: 0.8928\n",
            "Epoch 80/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7567 - val_loss: 0.8940\n",
            "Epoch 81/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7570 - val_loss: 0.8951\n",
            "Epoch 82/500\n",
            "105/105 [==============================] - 0s 135us/sample - loss: 0.7563 - val_loss: 0.9000\n",
            "Epoch 83/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7557 - val_loss: 0.9007\n",
            "Epoch 84/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7560 - val_loss: 0.9038\n",
            "Epoch 85/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7562 - val_loss: 0.9067\n",
            "Epoch 86/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7560 - val_loss: 0.9057\n",
            "Epoch 87/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7573 - val_loss: 0.9057\n",
            "Epoch 88/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7578 - val_loss: 0.9031\n",
            "Epoch 89/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7563 - val_loss: 0.9017\n",
            "Epoch 90/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7564 - val_loss: 0.9011\n",
            "Epoch 91/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7580 - val_loss: 0.9012\n",
            "Epoch 92/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7569 - val_loss: 0.8987\n",
            "Epoch 93/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7570 - val_loss: 0.8951\n",
            "Epoch 94/500\n",
            "105/105 [==============================] - 0s 186us/sample - loss: 0.7563 - val_loss: 0.8980\n",
            "Epoch 95/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7567 - val_loss: 0.8978\n",
            "Epoch 96/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7567 - val_loss: 0.8982\n",
            "Epoch 97/500\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.7574 - val_loss: 0.8960\n",
            "Epoch 98/500\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 0.7567 - val_loss: 0.8993\n",
            "Epoch 99/500\n",
            "105/105 [==============================] - 0s 170us/sample - loss: 0.7591 - val_loss: 0.9004\n",
            "Epoch 100/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7576 - val_loss: 0.8989\n",
            "Epoch 101/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7564 - val_loss: 0.8986\n",
            "Epoch 102/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7584 - val_loss: 0.8966\n",
            "Epoch 103/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7576 - val_loss: 0.8987\n",
            "Epoch 104/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7574 - val_loss: 0.8973\n",
            "Epoch 105/500\n",
            "105/105 [==============================] - 0s 170us/sample - loss: 0.7565 - val_loss: 0.8977\n",
            "Epoch 106/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7568 - val_loss: 0.9039\n",
            "Epoch 107/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7562 - val_loss: 0.9041\n",
            "Epoch 108/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7576 - val_loss: 0.9096\n",
            "Epoch 109/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7565 - val_loss: 0.9093\n",
            "Epoch 110/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7560 - val_loss: 0.9116\n",
            "Epoch 111/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7566 - val_loss: 0.9135\n",
            "Epoch 112/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7574 - val_loss: 0.9137\n",
            "Epoch 113/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7569 - val_loss: 0.9134\n",
            "Epoch 114/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7577 - val_loss: 0.9140\n",
            "Epoch 115/500\n",
            "105/105 [==============================] - 0s 135us/sample - loss: 0.7566 - val_loss: 0.9125\n",
            "Epoch 116/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7571 - val_loss: 0.9130\n",
            "Epoch 117/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7566 - val_loss: 0.9105\n",
            "Epoch 118/500\n",
            "105/105 [==============================] - 0s 170us/sample - loss: 0.7566 - val_loss: 0.9148\n",
            "Epoch 119/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7567 - val_loss: 0.9185\n",
            "Epoch 120/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7573 - val_loss: 0.9173\n",
            "Epoch 121/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7565 - val_loss: 0.9124\n",
            "Epoch 122/500\n",
            "105/105 [==============================] - 0s 129us/sample - loss: 0.7569 - val_loss: 0.9106\n",
            "Epoch 123/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7564 - val_loss: 0.9071\n",
            "Epoch 124/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7559 - val_loss: 0.9115\n",
            "Epoch 125/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7565 - val_loss: 0.9122\n",
            "Epoch 126/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7563 - val_loss: 0.9089\n",
            "Epoch 127/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7569 - val_loss: 0.9134\n",
            "Epoch 128/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7564 - val_loss: 0.9155\n",
            "Epoch 129/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7562 - val_loss: 0.9202\n",
            "Epoch 130/500\n",
            "105/105 [==============================] - 0s 179us/sample - loss: 0.7577 - val_loss: 0.9167\n",
            "Epoch 131/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7564 - val_loss: 0.9142\n",
            "Epoch 132/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7563 - val_loss: 0.9100\n",
            "Epoch 133/500\n",
            "105/105 [==============================] - 0s 188us/sample - loss: 0.7561 - val_loss: 0.9066\n",
            "Epoch 134/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7561 - val_loss: 0.9069\n",
            "Epoch 135/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7563 - val_loss: 0.9049\n",
            "Epoch 136/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7571 - val_loss: 0.9038\n",
            "Epoch 137/500\n",
            "105/105 [==============================] - 0s 134us/sample - loss: 0.7571 - val_loss: 0.9031\n",
            "Epoch 138/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7564 - val_loss: 0.9054\n",
            "Epoch 139/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7564 - val_loss: 0.9064\n",
            "Epoch 140/500\n",
            "105/105 [==============================] - 0s 135us/sample - loss: 0.7576 - val_loss: 0.9082\n",
            "Epoch 141/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7565 - val_loss: 0.9084\n",
            "Epoch 142/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7565 - val_loss: 0.9055\n",
            "Epoch 143/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7578 - val_loss: 0.9049\n",
            "Epoch 144/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7564 - val_loss: 0.9054\n",
            "Epoch 145/500\n",
            "105/105 [==============================] - 0s 126us/sample - loss: 0.7560 - val_loss: 0.9066\n",
            "Epoch 146/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7573 - val_loss: 0.9154\n",
            "Epoch 147/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7571 - val_loss: 0.9214\n",
            "Epoch 148/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7567 - val_loss: 0.9217\n",
            "Epoch 149/500\n",
            "105/105 [==============================] - 0s 170us/sample - loss: 0.7591 - val_loss: 0.9185\n",
            "Epoch 150/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7570 - val_loss: 0.9156\n",
            "Epoch 151/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7576 - val_loss: 0.9145\n",
            "Epoch 152/500\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.7571 - val_loss: 0.9164\n",
            "Epoch 153/500\n",
            "105/105 [==============================] - 0s 131us/sample - loss: 0.7573 - val_loss: 0.9223\n",
            "Epoch 154/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7579 - val_loss: 0.9231\n",
            "Epoch 155/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7581 - val_loss: 0.9229\n",
            "Epoch 156/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7586 - val_loss: 0.9208\n",
            "Epoch 157/500\n",
            "105/105 [==============================] - 0s 190us/sample - loss: 0.7573 - val_loss: 0.9137\n",
            "Epoch 158/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7566 - val_loss: 0.9145\n",
            "Epoch 159/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7565 - val_loss: 0.9144\n",
            "Epoch 160/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7583 - val_loss: 0.9092\n",
            "Epoch 161/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7568 - val_loss: 0.9107\n",
            "Epoch 162/500\n",
            "105/105 [==============================] - 0s 131us/sample - loss: 0.7557 - val_loss: 0.9108\n",
            "Epoch 163/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7571 - val_loss: 0.9125\n",
            "Epoch 164/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7574 - val_loss: 0.9203\n",
            "Epoch 165/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7583 - val_loss: 0.9152\n",
            "Epoch 166/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7567 - val_loss: 0.9145\n",
            "Epoch 167/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7561 - val_loss: 0.9134\n",
            "Epoch 168/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7568 - val_loss: 0.9177\n",
            "Epoch 169/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7565 - val_loss: 0.9167\n",
            "Epoch 170/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7565 - val_loss: 0.9140\n",
            "Epoch 171/500\n",
            "105/105 [==============================] - 0s 183us/sample - loss: 0.7570 - val_loss: 0.9130\n",
            "Epoch 172/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7566 - val_loss: 0.9117\n",
            "Epoch 173/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7572 - val_loss: 0.9092\n",
            "Epoch 174/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7564 - val_loss: 0.9083\n",
            "Epoch 175/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7565 - val_loss: 0.9116\n",
            "Epoch 176/500\n",
            "105/105 [==============================] - 0s 179us/sample - loss: 0.7561 - val_loss: 0.9124\n",
            "Epoch 177/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7574 - val_loss: 0.9145\n",
            "Epoch 178/500\n",
            "105/105 [==============================] - 0s 181us/sample - loss: 0.7572 - val_loss: 0.9141\n",
            "Epoch 179/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7570 - val_loss: 0.9174\n",
            "Epoch 180/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7580 - val_loss: 0.9136\n",
            "Epoch 181/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7570 - val_loss: 0.9093\n",
            "Epoch 182/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7575 - val_loss: 0.9059\n",
            "Epoch 183/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7561 - val_loss: 0.9040\n",
            "Epoch 184/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7561 - val_loss: 0.9037\n",
            "Epoch 185/500\n",
            "105/105 [==============================] - 0s 188us/sample - loss: 0.7560 - val_loss: 0.9039\n",
            "Epoch 186/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7567 - val_loss: 0.9058\n",
            "Epoch 187/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7563 - val_loss: 0.9088\n",
            "Epoch 188/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7581 - val_loss: 0.9094\n",
            "Epoch 189/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7569 - val_loss: 0.9053\n",
            "Epoch 190/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7559 - val_loss: 0.9075\n",
            "Epoch 191/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7565 - val_loss: 0.9073\n",
            "Epoch 192/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7558 - val_loss: 0.9106\n",
            "Epoch 193/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7571 - val_loss: 0.9084\n",
            "Epoch 194/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7562 - val_loss: 0.9107\n",
            "Epoch 195/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7574 - val_loss: 0.9108\n",
            "Epoch 196/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7560 - val_loss: 0.9128\n",
            "Epoch 197/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7565 - val_loss: 0.9085\n",
            "Epoch 198/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7559 - val_loss: 0.9077\n",
            "Epoch 199/500\n",
            "105/105 [==============================] - 0s 188us/sample - loss: 0.7558 - val_loss: 0.9111\n",
            "Epoch 200/500\n",
            "105/105 [==============================] - 0s 175us/sample - loss: 0.7560 - val_loss: 0.9111\n",
            "Epoch 201/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7586 - val_loss: 0.9132\n",
            "Epoch 202/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7569 - val_loss: 0.9113\n",
            "Epoch 203/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7560 - val_loss: 0.9127\n",
            "Epoch 204/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7567 - val_loss: 0.9158\n",
            "Epoch 205/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7580 - val_loss: 0.9204\n",
            "Epoch 206/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7567 - val_loss: 0.9226\n",
            "Epoch 207/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7577 - val_loss: 0.9215\n",
            "Epoch 208/500\n",
            "105/105 [==============================] - 0s 189us/sample - loss: 0.7579 - val_loss: 0.9169\n",
            "Epoch 209/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7573 - val_loss: 0.9138\n",
            "Epoch 210/500\n",
            "105/105 [==============================] - 0s 173us/sample - loss: 0.7574 - val_loss: 0.9138\n",
            "Epoch 211/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7569 - val_loss: 0.9165\n",
            "Epoch 212/500\n",
            "105/105 [==============================] - 0s 205us/sample - loss: 0.7563 - val_loss: 0.9125\n",
            "Epoch 213/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7576 - val_loss: 0.9054\n",
            "Epoch 214/500\n",
            "105/105 [==============================] - 0s 121us/sample - loss: 0.7562 - val_loss: 0.9017\n",
            "Epoch 215/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7564 - val_loss: 0.9033\n",
            "Epoch 216/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7567 - val_loss: 0.9025\n",
            "Epoch 217/500\n",
            "105/105 [==============================] - 0s 189us/sample - loss: 0.7557 - val_loss: 0.9042\n",
            "Epoch 218/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7562 - val_loss: 0.9030\n",
            "Epoch 219/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7559 - val_loss: 0.9035\n",
            "Epoch 220/500\n",
            "105/105 [==============================] - 0s 182us/sample - loss: 0.7562 - val_loss: 0.8986\n",
            "Epoch 221/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7575 - val_loss: 0.8987\n",
            "Epoch 222/500\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.7567 - val_loss: 0.8972\n",
            "Epoch 223/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7568 - val_loss: 0.8964\n",
            "Epoch 224/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7578 - val_loss: 0.8981\n",
            "Epoch 225/500\n",
            "105/105 [==============================] - 0s 178us/sample - loss: 0.7561 - val_loss: 0.8955\n",
            "Epoch 226/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7563 - val_loss: 0.8952\n",
            "Epoch 227/500\n",
            "105/105 [==============================] - 0s 192us/sample - loss: 0.7564 - val_loss: 0.9002\n",
            "Epoch 228/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7570 - val_loss: 0.9030\n",
            "Epoch 229/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7558 - val_loss: 0.9023\n",
            "Epoch 230/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7558 - val_loss: 0.9053\n",
            "Epoch 231/500\n",
            "105/105 [==============================] - 0s 176us/sample - loss: 0.7567 - val_loss: 0.9005\n",
            "Epoch 232/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7570 - val_loss: 0.9015\n",
            "Epoch 233/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7567 - val_loss: 0.8989\n",
            "Epoch 234/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7568 - val_loss: 0.8997\n",
            "Epoch 235/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7562 - val_loss: 0.9038\n",
            "Epoch 236/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7566 - val_loss: 0.9049\n",
            "Epoch 237/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7560 - val_loss: 0.9037\n",
            "Epoch 238/500\n",
            "105/105 [==============================] - 0s 206us/sample - loss: 0.7568 - val_loss: 0.9055\n",
            "Epoch 239/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7569 - val_loss: 0.9051\n",
            "Epoch 240/500\n",
            "105/105 [==============================] - 0s 180us/sample - loss: 0.7563 - val_loss: 0.9068\n",
            "Epoch 241/500\n",
            "105/105 [==============================] - 0s 168us/sample - loss: 0.7585 - val_loss: 0.9025\n",
            "Epoch 242/500\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.7569 - val_loss: 0.9060\n",
            "Epoch 243/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7563 - val_loss: 0.9017\n",
            "Epoch 244/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7561 - val_loss: 0.9060\n",
            "Epoch 245/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7586 - val_loss: 0.8995\n",
            "Epoch 246/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7574 - val_loss: 0.9002\n",
            "Epoch 247/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7559 - val_loss: 0.9008\n",
            "Epoch 248/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7576 - val_loss: 0.9105\n",
            "Epoch 249/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7570 - val_loss: 0.9128\n",
            "Epoch 250/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7561 - val_loss: 0.9111\n",
            "Epoch 251/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7575 - val_loss: 0.9069\n",
            "Epoch 252/500\n",
            "105/105 [==============================] - 0s 192us/sample - loss: 0.7572 - val_loss: 0.9067\n",
            "Epoch 253/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7561 - val_loss: 0.9037\n",
            "Epoch 254/500\n",
            "105/105 [==============================] - 0s 188us/sample - loss: 0.7562 - val_loss: 0.9020\n",
            "Epoch 255/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7564 - val_loss: 0.9008\n",
            "Epoch 256/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7567 - val_loss: 0.9008\n",
            "Epoch 257/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7564 - val_loss: 0.9006\n",
            "Epoch 258/500\n",
            "105/105 [==============================] - 0s 181us/sample - loss: 0.7568 - val_loss: 0.9037\n",
            "Epoch 259/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7562 - val_loss: 0.9044\n",
            "Epoch 260/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7575 - val_loss: 0.9039\n",
            "Epoch 261/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7572 - val_loss: 0.9036\n",
            "Epoch 262/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7573 - val_loss: 0.9075\n",
            "Epoch 263/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7563 - val_loss: 0.9092\n",
            "Epoch 264/500\n",
            "105/105 [==============================] - 0s 192us/sample - loss: 0.7573 - val_loss: 0.9070\n",
            "Epoch 265/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7565 - val_loss: 0.9075\n",
            "Epoch 266/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7563 - val_loss: 0.9126\n",
            "Epoch 267/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7587 - val_loss: 0.9116\n",
            "Epoch 268/500\n",
            "105/105 [==============================] - 0s 193us/sample - loss: 0.7559 - val_loss: 0.9101\n",
            "Epoch 269/500\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.7575 - val_loss: 0.9114\n",
            "Epoch 270/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7559 - val_loss: 0.9119\n",
            "Epoch 271/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7572 - val_loss: 0.9064\n",
            "Epoch 272/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7561 - val_loss: 0.9044\n",
            "Epoch 273/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7564 - val_loss: 0.9035\n",
            "Epoch 274/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7567 - val_loss: 0.9026\n",
            "Epoch 275/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7579 - val_loss: 0.9022\n",
            "Epoch 276/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7556 - val_loss: 0.9048\n",
            "Epoch 277/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7567 - val_loss: 0.9046\n",
            "Epoch 278/500\n",
            "105/105 [==============================] - 0s 174us/sample - loss: 0.7560 - val_loss: 0.9036\n",
            "Epoch 279/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7569 - val_loss: 0.9051\n",
            "Epoch 280/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7563 - val_loss: 0.9038\n",
            "Epoch 281/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7558 - val_loss: 0.9021\n",
            "Epoch 282/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7560 - val_loss: 0.9021\n",
            "Epoch 283/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7569 - val_loss: 0.9077\n",
            "Epoch 284/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7560 - val_loss: 0.9102\n",
            "Epoch 285/500\n",
            "105/105 [==============================] - 0s 178us/sample - loss: 0.7571 - val_loss: 0.9095\n",
            "Epoch 286/500\n",
            "105/105 [==============================] - 0s 168us/sample - loss: 0.7574 - val_loss: 0.9125\n",
            "Epoch 287/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7574 - val_loss: 0.9196\n",
            "Epoch 288/500\n",
            "105/105 [==============================] - 0s 185us/sample - loss: 0.7583 - val_loss: 0.9226\n",
            "Epoch 289/500\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.7571 - val_loss: 0.9195\n",
            "Epoch 290/500\n",
            "105/105 [==============================] - 0s 168us/sample - loss: 0.7567 - val_loss: 0.9239\n",
            "Epoch 291/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7572 - val_loss: 0.9278\n",
            "Epoch 292/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7595 - val_loss: 0.9340\n",
            "Epoch 293/500\n",
            "105/105 [==============================] - 0s 168us/sample - loss: 0.7586 - val_loss: 0.9338\n",
            "Epoch 294/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7594 - val_loss: 0.9336\n",
            "Epoch 295/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7589 - val_loss: 0.9334\n",
            "Epoch 296/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7621 - val_loss: 0.9271\n",
            "Epoch 297/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7587 - val_loss: 0.9271\n",
            "Epoch 298/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7588 - val_loss: 0.9243\n",
            "Epoch 299/500\n",
            "105/105 [==============================] - 0s 135us/sample - loss: 0.7574 - val_loss: 0.9234\n",
            "Epoch 300/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7578 - val_loss: 0.9217\n",
            "Epoch 301/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7572 - val_loss: 0.9194\n",
            "Epoch 302/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7566 - val_loss: 0.9167\n",
            "Epoch 303/500\n",
            "105/105 [==============================] - 0s 178us/sample - loss: 0.7561 - val_loss: 0.9167\n",
            "Epoch 304/500\n",
            "105/105 [==============================] - 0s 136us/sample - loss: 0.7580 - val_loss: 0.9175\n",
            "Epoch 305/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7573 - val_loss: 0.9148\n",
            "Epoch 306/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7571 - val_loss: 0.9103\n",
            "Epoch 307/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7561 - val_loss: 0.9147\n",
            "Epoch 308/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7562 - val_loss: 0.9120\n",
            "Epoch 309/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7567 - val_loss: 0.9110\n",
            "Epoch 310/500\n",
            "105/105 [==============================] - 0s 131us/sample - loss: 0.7562 - val_loss: 0.9147\n",
            "Epoch 311/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7575 - val_loss: 0.9162\n",
            "Epoch 312/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7576 - val_loss: 0.9135\n",
            "Epoch 313/500\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.7567 - val_loss: 0.9169\n",
            "Epoch 314/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7566 - val_loss: 0.9188\n",
            "Epoch 315/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7576 - val_loss: 0.9174\n",
            "Epoch 316/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7566 - val_loss: 0.9192\n",
            "Epoch 317/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7573 - val_loss: 0.9159\n",
            "Epoch 318/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7569 - val_loss: 0.9118\n",
            "Epoch 319/500\n",
            "105/105 [==============================] - 0s 187us/sample - loss: 0.7567 - val_loss: 0.9133\n",
            "Epoch 320/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7563 - val_loss: 0.9121\n",
            "Epoch 321/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7561 - val_loss: 0.9153\n",
            "Epoch 322/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7566 - val_loss: 0.9163\n",
            "Epoch 323/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7583 - val_loss: 0.9108\n",
            "Epoch 324/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7559 - val_loss: 0.9106\n",
            "Epoch 325/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7559 - val_loss: 0.9092\n",
            "Epoch 326/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7568 - val_loss: 0.9086\n",
            "Epoch 327/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7566 - val_loss: 0.9092\n",
            "Epoch 328/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7567 - val_loss: 0.9054\n",
            "Epoch 329/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7574 - val_loss: 0.9093\n",
            "Epoch 330/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7563 - val_loss: 0.9093\n",
            "Epoch 331/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7560 - val_loss: 0.9077\n",
            "Epoch 332/500\n",
            "105/105 [==============================] - 0s 138us/sample - loss: 0.7561 - val_loss: 0.9107\n",
            "Epoch 333/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7571 - val_loss: 0.9092\n",
            "Epoch 334/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7563 - val_loss: 0.9133\n",
            "Epoch 335/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7574 - val_loss: 0.9124\n",
            "Epoch 336/500\n",
            "105/105 [==============================] - 0s 135us/sample - loss: 0.7567 - val_loss: 0.9122\n",
            "Epoch 337/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7571 - val_loss: 0.9149\n",
            "Epoch 338/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7567 - val_loss: 0.9104\n",
            "Epoch 339/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7561 - val_loss: 0.9087\n",
            "Epoch 340/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7581 - val_loss: 0.9093\n",
            "Epoch 341/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7561 - val_loss: 0.9074\n",
            "Epoch 342/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7559 - val_loss: 0.9072\n",
            "Epoch 343/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7567 - val_loss: 0.9075\n",
            "Epoch 344/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7565 - val_loss: 0.9107\n",
            "Epoch 345/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7560 - val_loss: 0.9076\n",
            "Epoch 346/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7561 - val_loss: 0.9058\n",
            "Epoch 347/500\n",
            "105/105 [==============================] - 0s 173us/sample - loss: 0.7561 - val_loss: 0.9039\n",
            "Epoch 348/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7563 - val_loss: 0.9061\n",
            "Epoch 349/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7566 - val_loss: 0.9050\n",
            "Epoch 350/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7560 - val_loss: 0.9051\n",
            "Epoch 351/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7561 - val_loss: 0.9065\n",
            "Epoch 352/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7558 - val_loss: 0.9092\n",
            "Epoch 353/500\n",
            "105/105 [==============================] - 0s 172us/sample - loss: 0.7565 - val_loss: 0.9092\n",
            "Epoch 354/500\n",
            "105/105 [==============================] - 0s 159us/sample - loss: 0.7568 - val_loss: 0.9094\n",
            "Epoch 355/500\n",
            "105/105 [==============================] - 0s 142us/sample - loss: 0.7564 - val_loss: 0.9149\n",
            "Epoch 356/500\n",
            "105/105 [==============================] - 0s 225us/sample - loss: 0.7569 - val_loss: 0.9137\n",
            "Epoch 357/500\n",
            "105/105 [==============================] - 0s 164us/sample - loss: 0.7577 - val_loss: 0.9084\n",
            "Epoch 358/500\n",
            "105/105 [==============================] - 0s 193us/sample - loss: 0.7566 - val_loss: 0.9051\n",
            "Epoch 359/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7561 - val_loss: 0.9073\n",
            "Epoch 360/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7564 - val_loss: 0.9107\n",
            "Epoch 361/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7561 - val_loss: 0.9054\n",
            "Epoch 362/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7568 - val_loss: 0.9120\n",
            "Epoch 363/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7580 - val_loss: 0.9095\n",
            "Epoch 364/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7561 - val_loss: 0.9091\n",
            "Epoch 365/500\n",
            "105/105 [==============================] - 0s 183us/sample - loss: 0.7561 - val_loss: 0.9077\n",
            "Epoch 366/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7566 - val_loss: 0.9117\n",
            "Epoch 367/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7560 - val_loss: 0.9101\n",
            "Epoch 368/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7569 - val_loss: 0.9070\n",
            "Epoch 369/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7563 - val_loss: 0.9052\n",
            "Epoch 370/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7573 - val_loss: 0.9026\n",
            "Epoch 371/500\n",
            "105/105 [==============================] - 0s 192us/sample - loss: 0.7560 - val_loss: 0.9019\n",
            "Epoch 372/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7569 - val_loss: 0.8992\n",
            "Epoch 373/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7565 - val_loss: 0.9000\n",
            "Epoch 374/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7576 - val_loss: 0.8995\n",
            "Epoch 375/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7561 - val_loss: 0.9029\n",
            "Epoch 376/500\n",
            "105/105 [==============================] - 0s 135us/sample - loss: 0.7568 - val_loss: 0.9014\n",
            "Epoch 377/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7565 - val_loss: 0.8992\n",
            "Epoch 378/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7559 - val_loss: 0.8973\n",
            "Epoch 379/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7569 - val_loss: 0.8989\n",
            "Epoch 380/500\n",
            "105/105 [==============================] - 0s 136us/sample - loss: 0.7557 - val_loss: 0.8984\n",
            "Epoch 381/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7562 - val_loss: 0.8942\n",
            "Epoch 382/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7567 - val_loss: 0.8920\n",
            "Epoch 383/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7591 - val_loss: 0.8940\n",
            "Epoch 384/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7575 - val_loss: 0.8938\n",
            "Epoch 385/500\n",
            "105/105 [==============================] - 0s 198us/sample - loss: 0.7571 - val_loss: 0.8943\n",
            "Epoch 386/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7568 - val_loss: 0.8995\n",
            "Epoch 387/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7575 - val_loss: 0.8995\n",
            "Epoch 388/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7577 - val_loss: 0.8986\n",
            "Epoch 389/500\n",
            "105/105 [==============================] - 0s 134us/sample - loss: 0.7568 - val_loss: 0.9010\n",
            "Epoch 390/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7569 - val_loss: 0.9048\n",
            "Epoch 391/500\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.7559 - val_loss: 0.9046\n",
            "Epoch 392/500\n",
            "105/105 [==============================] - 0s 135us/sample - loss: 0.7558 - val_loss: 0.9058\n",
            "Epoch 393/500\n",
            "105/105 [==============================] - 0s 123us/sample - loss: 0.7558 - val_loss: 0.9039\n",
            "Epoch 394/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7562 - val_loss: 0.9082\n",
            "Epoch 395/500\n",
            "105/105 [==============================] - 0s 166us/sample - loss: 0.7562 - val_loss: 0.9036\n",
            "Epoch 396/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7560 - val_loss: 0.9050\n",
            "Epoch 397/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7561 - val_loss: 0.9072\n",
            "Epoch 398/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7560 - val_loss: 0.9105\n",
            "Epoch 399/500\n",
            "105/105 [==============================] - 0s 130us/sample - loss: 0.7577 - val_loss: 0.9067\n",
            "Epoch 400/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7562 - val_loss: 0.9032\n",
            "Epoch 401/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7574 - val_loss: 0.9016\n",
            "Epoch 402/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7559 - val_loss: 0.9026\n",
            "Epoch 403/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7560 - val_loss: 0.9033\n",
            "Epoch 404/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7557 - val_loss: 0.9019\n",
            "Epoch 405/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7574 - val_loss: 0.9044\n",
            "Epoch 406/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7555 - val_loss: 0.9054\n",
            "Epoch 407/500\n",
            "105/105 [==============================] - 0s 143us/sample - loss: 0.7562 - val_loss: 0.9035\n",
            "Epoch 408/500\n",
            "105/105 [==============================] - 0s 136us/sample - loss: 0.7555 - val_loss: 0.9029\n",
            "Epoch 409/500\n",
            "105/105 [==============================] - 0s 136us/sample - loss: 0.7570 - val_loss: 0.9013\n",
            "Epoch 410/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7558 - val_loss: 0.9000\n",
            "Epoch 411/500\n",
            "105/105 [==============================] - 0s 137us/sample - loss: 0.7576 - val_loss: 0.9043\n",
            "Epoch 412/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7559 - val_loss: 0.9086\n",
            "Epoch 413/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7582 - val_loss: 0.9051\n",
            "Epoch 414/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7563 - val_loss: 0.9036\n",
            "Epoch 415/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7565 - val_loss: 0.9072\n",
            "Epoch 416/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7561 - val_loss: 0.9071\n",
            "Epoch 417/500\n",
            "105/105 [==============================] - 0s 203us/sample - loss: 0.7562 - val_loss: 0.9068\n",
            "Epoch 418/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7570 - val_loss: 0.9050\n",
            "Epoch 419/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7557 - val_loss: 0.9031\n",
            "Epoch 420/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7561 - val_loss: 0.9028\n",
            "Epoch 421/500\n",
            "105/105 [==============================] - 0s 127us/sample - loss: 0.7564 - val_loss: 0.9018\n",
            "Epoch 422/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7572 - val_loss: 0.9049\n",
            "Epoch 423/500\n",
            "105/105 [==============================] - 0s 167us/sample - loss: 0.7561 - val_loss: 0.9053\n",
            "Epoch 424/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7566 - val_loss: 0.9061\n",
            "Epoch 425/500\n",
            "105/105 [==============================] - 0s 151us/sample - loss: 0.7564 - val_loss: 0.9069\n",
            "Epoch 426/500\n",
            "105/105 [==============================] - 0s 139us/sample - loss: 0.7564 - val_loss: 0.9075\n",
            "Epoch 427/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7566 - val_loss: 0.9089\n",
            "Epoch 428/500\n",
            "105/105 [==============================] - 0s 169us/sample - loss: 0.7567 - val_loss: 0.9123\n",
            "Epoch 429/500\n",
            "105/105 [==============================] - 0s 162us/sample - loss: 0.7567 - val_loss: 0.9127\n",
            "Epoch 430/500\n",
            "105/105 [==============================] - 0s 177us/sample - loss: 0.7560 - val_loss: 0.9098\n",
            "Epoch 431/500\n",
            "105/105 [==============================] - 0s 212us/sample - loss: 0.7567 - val_loss: 0.9045\n",
            "Epoch 432/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7561 - val_loss: 0.9085\n",
            "Epoch 433/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7561 - val_loss: 0.9069\n",
            "Epoch 434/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7562 - val_loss: 0.9037\n",
            "Epoch 435/500\n",
            "105/105 [==============================] - 0s 131us/sample - loss: 0.7563 - val_loss: 0.9015\n",
            "Epoch 436/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7565 - val_loss: 0.8980\n",
            "Epoch 437/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7566 - val_loss: 0.8978\n",
            "Epoch 438/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7569 - val_loss: 0.9000\n",
            "Epoch 439/500\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.7567 - val_loss: 0.9003\n",
            "Epoch 440/500\n",
            "105/105 [==============================] - 0s 146us/sample - loss: 0.7570 - val_loss: 0.9006\n",
            "Epoch 441/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7570 - val_loss: 0.8986\n",
            "Epoch 442/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7564 - val_loss: 0.8991\n",
            "Epoch 443/500\n",
            "105/105 [==============================] - 0s 183us/sample - loss: 0.7564 - val_loss: 0.9009\n",
            "Epoch 444/500\n",
            "105/105 [==============================] - 0s 169us/sample - loss: 0.7561 - val_loss: 0.9003\n",
            "Epoch 445/500\n",
            "105/105 [==============================] - 0s 155us/sample - loss: 0.7566 - val_loss: 0.8996\n",
            "Epoch 446/500\n",
            "105/105 [==============================] - 0s 168us/sample - loss: 0.7565 - val_loss: 0.8999\n",
            "Epoch 447/500\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.7564 - val_loss: 0.8992\n",
            "Epoch 448/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7561 - val_loss: 0.9004\n",
            "Epoch 449/500\n",
            "105/105 [==============================] - 0s 178us/sample - loss: 0.7562 - val_loss: 0.9040\n",
            "Epoch 450/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7563 - val_loss: 0.8996\n",
            "Epoch 451/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7563 - val_loss: 0.9028\n",
            "Epoch 452/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7570 - val_loss: 0.9034\n",
            "Epoch 453/500\n",
            "105/105 [==============================] - 0s 126us/sample - loss: 0.7566 - val_loss: 0.9031\n",
            "Epoch 454/500\n",
            "105/105 [==============================] - 0s 125us/sample - loss: 0.7565 - val_loss: 0.9059\n",
            "Epoch 455/500\n",
            "105/105 [==============================] - 0s 134us/sample - loss: 0.7565 - val_loss: 0.9009\n",
            "Epoch 456/500\n",
            "105/105 [==============================] - 0s 171us/sample - loss: 0.7564 - val_loss: 0.9034\n",
            "Epoch 457/500\n",
            "105/105 [==============================] - 0s 165us/sample - loss: 0.7562 - val_loss: 0.9023\n",
            "Epoch 458/500\n",
            "105/105 [==============================] - 0s 141us/sample - loss: 0.7562 - val_loss: 0.9056\n",
            "Epoch 459/500\n",
            "105/105 [==============================] - 0s 189us/sample - loss: 0.7577 - val_loss: 0.9053\n",
            "Epoch 460/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7564 - val_loss: 0.9051\n",
            "Epoch 461/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7559 - val_loss: 0.9040\n",
            "Epoch 462/500\n",
            "105/105 [==============================] - 0s 147us/sample - loss: 0.7556 - val_loss: 0.9024\n",
            "Epoch 463/500\n",
            "105/105 [==============================] - 0s 150us/sample - loss: 0.7569 - val_loss: 0.9106\n",
            "Epoch 464/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7562 - val_loss: 0.9121\n",
            "Epoch 465/500\n",
            "105/105 [==============================] - 0s 145us/sample - loss: 0.7568 - val_loss: 0.9137\n",
            "Epoch 466/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7576 - val_loss: 0.9092\n",
            "Epoch 467/500\n",
            "105/105 [==============================] - 0s 133us/sample - loss: 0.7580 - val_loss: 0.9079\n",
            "Epoch 468/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7562 - val_loss: 0.9111\n",
            "Epoch 469/500\n",
            "105/105 [==============================] - 0s 154us/sample - loss: 0.7582 - val_loss: 0.9166\n",
            "Epoch 470/500\n",
            "105/105 [==============================] - 0s 130us/sample - loss: 0.7579 - val_loss: 0.9132\n",
            "Epoch 471/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7565 - val_loss: 0.9097\n",
            "Epoch 472/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7567 - val_loss: 0.9103\n",
            "Epoch 473/500\n",
            "105/105 [==============================] - 0s 219us/sample - loss: 0.7562 - val_loss: 0.9069\n",
            "Epoch 474/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7562 - val_loss: 0.9126\n",
            "Epoch 475/500\n",
            "105/105 [==============================] - 0s 144us/sample - loss: 0.7570 - val_loss: 0.9117\n",
            "Epoch 476/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7580 - val_loss: 0.9097\n",
            "Epoch 477/500\n",
            "105/105 [==============================] - 0s 161us/sample - loss: 0.7560 - val_loss: 0.9127\n",
            "Epoch 478/500\n",
            "105/105 [==============================] - 0s 168us/sample - loss: 0.7561 - val_loss: 0.9128\n",
            "Epoch 479/500\n",
            "105/105 [==============================] - 0s 156us/sample - loss: 0.7568 - val_loss: 0.9126\n",
            "Epoch 480/500\n",
            "105/105 [==============================] - 0s 149us/sample - loss: 0.7576 - val_loss: 0.9097\n",
            "Epoch 481/500\n",
            "105/105 [==============================] - 0s 135us/sample - loss: 0.7569 - val_loss: 0.9086\n",
            "Epoch 482/500\n",
            "105/105 [==============================] - 0s 173us/sample - loss: 0.7568 - val_loss: 0.9086\n",
            "Epoch 483/500\n",
            "105/105 [==============================] - 0s 158us/sample - loss: 0.7569 - val_loss: 0.9170\n",
            "Epoch 484/500\n",
            "105/105 [==============================] - 0s 136us/sample - loss: 0.7568 - val_loss: 0.9132\n",
            "Epoch 485/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7570 - val_loss: 0.9097\n",
            "Epoch 486/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7572 - val_loss: 0.9046\n",
            "Epoch 487/500\n",
            "105/105 [==============================] - 0s 173us/sample - loss: 0.7568 - val_loss: 0.9066\n",
            "Epoch 488/500\n",
            "105/105 [==============================] - 0s 183us/sample - loss: 0.7568 - val_loss: 0.9065\n",
            "Epoch 489/500\n",
            "105/105 [==============================] - 0s 195us/sample - loss: 0.7572 - val_loss: 0.9138\n",
            "Epoch 490/500\n",
            "105/105 [==============================] - 0s 148us/sample - loss: 0.7576 - val_loss: 0.9101\n",
            "Epoch 491/500\n",
            "105/105 [==============================] - 0s 179us/sample - loss: 0.7565 - val_loss: 0.9134\n",
            "Epoch 492/500\n",
            "105/105 [==============================] - 0s 157us/sample - loss: 0.7575 - val_loss: 0.9187\n",
            "Epoch 493/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7568 - val_loss: 0.9190\n",
            "Epoch 494/500\n",
            "105/105 [==============================] - 0s 163us/sample - loss: 0.7568 - val_loss: 0.9150\n",
            "Epoch 495/500\n",
            "105/105 [==============================] - 0s 152us/sample - loss: 0.7561 - val_loss: 0.9164\n",
            "Epoch 496/500\n",
            "105/105 [==============================] - 0s 140us/sample - loss: 0.7568 - val_loss: 0.9186\n",
            "Epoch 497/500\n",
            "105/105 [==============================] - 0s 135us/sample - loss: 0.7576 - val_loss: 0.9163\n",
            "Epoch 498/500\n",
            "105/105 [==============================] - 0s 160us/sample - loss: 0.7605 - val_loss: 0.9198\n",
            "Epoch 499/500\n",
            "105/105 [==============================] - 0s 153us/sample - loss: 0.7571 - val_loss: 0.9191\n",
            "Epoch 500/500\n",
            "105/105 [==============================] - 0s 181us/sample - loss: 0.7569 - val_loss: 0.9167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBuaN-5gu_eq",
        "colab_type": "code",
        "outputId": "04bf19c8-21ba-4442-970f-4b9b7be3516e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(epochs, history.history['loss'], label = 'Training loss')\n",
        "plt.plot(epochs, history.history['val_loss'], label = 'Validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ???? validation loss가 뭔가 이상하다...? ㅠㅠ 왜이래 또..."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mqrp1kTvmHO",
        "colab_type": "text"
      },
      "source": [
        "## tf.keras모델의 저장 & 복원"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pbu-noEvPeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save_weight 메소드 이용(현재폴더에 파일 생성 후 모든 가중치를 저장)\n",
        "model.save_weights('simple_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HsscylMv5u6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load_weights메소드를 사용하여 복원\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units = 1, input_dim = 1))\n",
        "model.compile(optimizer = 'sgd', loss = 'mse')\n",
        "\n",
        "model.load_weights('simple_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKwWzHRuwQaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save 메소드를 사용하여 모델 전체를 저장\n",
        "model.save('simple_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSaXdaKlwklW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load_model메소드를 이용하여 모델 복원\n",
        "model = tf.keras.models.load_model('simple_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs2CglWhxE31",
        "colab_type": "text"
      },
      "source": [
        "### ModelCheckpoint\n",
        "모델을 훈련하면서 최고의 가중치를 저장할 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgGjML6zwxzz",
        "colab_type": "code",
        "outputId": "80b81419-38c0-4e6a-871b-6da7e2c7fb32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units = 1, input_dim = 1))\n",
        "model.compile(optimizer = 'sgd', loss = 'mse')\n",
        "\n",
        "callback_list = [tf.keras.callbacks.ModelCheckpoint(filepath = 'my_model.h5', monitor = 'val_loss',\n",
        "                                                    save_best_only = True),\n",
        "                 tf.keras.callbacks.EarlyStopping(patience = 5)]\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs = 500, validation_split = 0.2, callbacks = callback_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 120 samples, validate on 30 samples\n",
            "Epoch 1/500\n",
            "120/120 [==============================] - 0s 2ms/sample - loss: 1.6717 - val_loss: 1.2468\n",
            "Epoch 2/500\n",
            "120/120 [==============================] - 0s 178us/sample - loss: 1.5313 - val_loss: 1.1597\n",
            "Epoch 3/500\n",
            "120/120 [==============================] - 0s 171us/sample - loss: 1.4166 - val_loss: 1.0901\n",
            "Epoch 4/500\n",
            "120/120 [==============================] - 0s 181us/sample - loss: 1.3225 - val_loss: 1.0329\n",
            "Epoch 5/500\n",
            "120/120 [==============================] - 0s 190us/sample - loss: 1.2402 - val_loss: 0.9868\n",
            "Epoch 6/500\n",
            "120/120 [==============================] - 0s 192us/sample - loss: 1.1735 - val_loss: 0.9496\n",
            "Epoch 7/500\n",
            "120/120 [==============================] - 0s 192us/sample - loss: 1.1201 - val_loss: 0.9159\n",
            "Epoch 8/500\n",
            "120/120 [==============================] - 0s 171us/sample - loss: 1.0683 - val_loss: 0.8895\n",
            "Epoch 9/500\n",
            "120/120 [==============================] - 0s 212us/sample - loss: 1.0255 - val_loss: 0.8694\n",
            "Epoch 10/500\n",
            "120/120 [==============================] - 0s 206us/sample - loss: 0.9932 - val_loss: 0.8522\n",
            "Epoch 11/500\n",
            "120/120 [==============================] - 0s 222us/sample - loss: 0.9608 - val_loss: 0.8389\n",
            "Epoch 12/500\n",
            "120/120 [==============================] - 0s 185us/sample - loss: 0.9354 - val_loss: 0.8291\n",
            "Epoch 13/500\n",
            "120/120 [==============================] - 0s 196us/sample - loss: 0.9143 - val_loss: 0.8218\n",
            "Epoch 14/500\n",
            "120/120 [==============================] - 0s 197us/sample - loss: 0.8948 - val_loss: 0.8153\n",
            "Epoch 15/500\n",
            "120/120 [==============================] - 0s 181us/sample - loss: 0.8798 - val_loss: 0.8110\n",
            "Epoch 16/500\n",
            "120/120 [==============================] - 0s 213us/sample - loss: 0.8661 - val_loss: 0.8072\n",
            "Epoch 17/500\n",
            "120/120 [==============================] - 0s 163us/sample - loss: 0.8547 - val_loss: 0.8041\n",
            "Epoch 18/500\n",
            "120/120 [==============================] - 0s 197us/sample - loss: 0.8453 - val_loss: 0.8034\n",
            "Epoch 19/500\n",
            "120/120 [==============================] - 0s 231us/sample - loss: 0.8351 - val_loss: 0.8027\n",
            "Epoch 20/500\n",
            "120/120 [==============================] - 0s 189us/sample - loss: 0.8284 - val_loss: 0.8026\n",
            "Epoch 21/500\n",
            "120/120 [==============================] - 0s 139us/sample - loss: 0.8244 - val_loss: 0.8034\n",
            "Epoch 22/500\n",
            "120/120 [==============================] - 0s 132us/sample - loss: 0.8175 - val_loss: 0.8043\n",
            "Epoch 23/500\n",
            "120/120 [==============================] - 0s 131us/sample - loss: 0.8141 - val_loss: 0.8051\n",
            "Epoch 24/500\n",
            "120/120 [==============================] - 0s 134us/sample - loss: 0.8102 - val_loss: 0.8056\n",
            "Epoch 25/500\n",
            "120/120 [==============================] - 0s 129us/sample - loss: 0.8068 - val_loss: 0.8071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yhbkbyMxzME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = np.arange(1, len(history.history['loss']) + 1)\n",
        "plt.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QNBw9QFzb_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUd3UFHyzb9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRoTUXbgzb6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YDhUkgUzb30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye1hP013zc_o",
        "colab_type": "text"
      },
      "source": [
        "## 케라스 층 그래프 그리기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9BFenBTzb0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = tf.keras.Input(shape = (784,))\n",
        "hidden = tf.keras.layers.Dense(100)(input)\n",
        "output = tf.keras.layers.Dense(10)(hidden)\n",
        "\n",
        "model = tf.keras.Model(input, output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1R7wtdvztLY",
        "colab_type": "code",
        "outputId": "376e6d96-ecbf-447a-e221-78ae0a0baac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# plot_model 함수 사용\n",
        "# to_file의 parameter로는 저장할 파일 이름\n",
        "tf.keras.utils.plot_model(model, show_shapes = True, to_file = 'model_1.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEnCAYAAACE69lsAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXUhdiG1z6NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}