{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "activation_function.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMATps5YSyUK23xze5LgE3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverstar0727/study-/blob/master/activation_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYSn7gpyzH84",
        "colab_type": "text"
      },
      "source": [
        "# Activation function\n",
        "\n",
        "## Activation 3가지 분류\n",
        "* Binary step function\n",
        "\n",
        "임계치를 기준으로 출력... perceptron 알고리즘에서 활성화 함수로 사용함 0 or 1 다중분류에서 다중 출력을 할 수 없다는 단점이 존재\n",
        "\n",
        "* Linear activation function\n",
        "\n",
        "특정 상수를 곱한 값을 출력으로... back propagation에서 미분할 때 상수이므로 역전파가 안된다는 것과, 은닉층을 무시할 수 있다는 단점이 존재\n",
        "\n",
        "* non linear activation function\n",
        "\n",
        "따라서 비선형 함수를 사용함. 미분가능과 역전파의 가능, 심층 신경망을 통한 더 많은 정보추출의 장점이 존재\n",
        "\n",
        "## Non-linear activation의 종류\n",
        "\n",
        "* sigmoid\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/49096513/91440812-23260500-e8aa-11ea-8dd7-7d99eaff309c.png)\n",
        "\n",
        "  * pros\n",
        "\n",
        "  1. 유연한 미분값을 가짐\n",
        "  2. 출력값의 범위가 0~1로 제한 -> 정규화에서 exploding gradient 문제를 방지함\n",
        "  3. 미분이 단순한 형태\n",
        "\n",
        "  * cons\n",
        "  1. vanishing gradient문제가 발생함(층이 쌓일수록 gradient 값이 0에 수렴)\n",
        "  2. 출력의 중심이 0이 아님(일단 그렇다고 이해..)\n",
        "  3. exp연산에대한 큰 비용\n",
        "\n",
        "* tanh\n",
        "\n",
        "sigmoid의 개선버전\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/49096513/91440711-f671ed80-e8a9-11ea-9294-371dbc7ad955.png)\n",
        "\n",
        "\n",
        "  * pros\n",
        "  1. 0이 중심에 있음 (sigmoid의 단점 해결)\n",
        "  2. 다른 장점은 sigmoid와 동일\n",
        "  * cons\n",
        "  1. center 문제를 제외하곤 sigmoid와 동일\n",
        "\n",
        "\n",
        "* ReLU\n",
        "\n",
        "Rectified Linear Unit\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/49096513/91441907-f83cb080-e8ab-11ea-8f53-1d6b2dfb47c2.png)\n",
        "\n",
        "\n",
        "  * pros\n",
        "  1. 빠른 연산(비교연산 1회를 통해 함숫값 도출가능)\n",
        "  2. 비선형(역전파 가능)\n",
        "  * cons\n",
        "  1. dying relu(입력이 0 이하일때 gradient는 0이 되어 학습하지 못함)\n",
        "\n",
        "* softmax\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/49096513/91442028-3043f380-e8ac-11ea-987d-8b9be73245f3.png)\n",
        "\n",
        "  * pros\n",
        "  1. 다중 클래스 문제에 적용 가능\n",
        "  2. 정규화 기능을 지님\n",
        "\n",
        "  * cons\n",
        "  1. 지수함수를 이용하여 오버플로 발생 가능성 존재\n",
        "\n",
        "여기까지는 보편적인 activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhwp6-2mZxnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLKoB2qP5LRa",
        "colab_type": "text"
      },
      "source": [
        "## 추가적인 activation function\n",
        "* leaky ReLU\n",
        "\n",
        "음수부에 작은 상수를 곱하여 dying relu문제를 피함\n",
        "![image](https://user-images.githubusercontent.com/49096513/91442236-7d27ca00-e8ac-11ea-9f5b-d4c1fc0336e1.png)\n",
        "\n",
        "  * pros\n",
        "  1. dying relu문제를 방지함\n",
        "  2. 연산이 빠름\n",
        "  3. 균형적인 값을 반환함(relu에 비하여)\n",
        "   \n",
        "  * cons\n",
        "  1. relu보다 항상 나은 성능을 갖는 것은 아님\n",
        "\n",
        "* ELU\n",
        "\n",
        "exponential linear unit을 의미\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/49096513/91442580-07702e00-e8ad-11ea-87e5-5b186d63d932.png)\n",
        "\n",
        "  * pros\n",
        "  1. relu의 모든 장점을 포함함\n",
        "  2. dying relu를 해결함\n",
        "\n",
        "  * cons\n",
        "  1. exp함수를 사용하여 큰 비용\n",
        "  2. 큰 음수값에 대해서 쉽게 포화됨\n",
        "\n",
        "* swish\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/49096513/91442635-1eaf1b80-e8ad-11ea-8b0f-1849b2cf810c.png)\n",
        "\n",
        "relu보다 부드러운 형태를 지님\n",
        "\n",
        "* Maxout\n",
        "\n",
        "softmax와 마찬가지로 출력이 여러개\n",
        "\n",
        "\n",
        "  * pros\n",
        "  1. relu의 장점을 지님\n",
        "  2. 좋은 성능\n",
        "  3. dropout과 함께 사용하기에 효과적인 함수\n",
        "  * cons\n",
        "  1. 계산량이 많고 복잡함\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "183SR2bV5K1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv7o82OiyFnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
